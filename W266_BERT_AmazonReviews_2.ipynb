{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Yoks35UT5or"
   },
   "source": [
    "# Predicting the Helpfulness of Amazon Reviews\n",
    "### Keane Johnson and Tucker Anderson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds multiple models that predict the helpfulness of Amazon reviews. It uses the [2015 Amazon Review dataset](http://jmcauley.ucsd.edu/data/amazon/index.html), compiled by Julian McAuley, associate professor in the Computer Science department at the University of California, San Diego.\n",
    "\n",
    "The dataset contains product reviews from Amazon from May 1996 - July 2014, and includes ratings, text, helpfulness votes, descriptions, category information, price, brand, and image features. It is broken into smaller subsets, organized by product category.\n",
    "\n",
    "This notebook focuses on the Home and Kitchen sub-category, and uses the aforementioned features to predict helpfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- Import Libraries\n",
    "- Load and Prepare Dataset\n",
    "- Exploratory Data Analysis\n",
    "- Model 1: Every review is 100% helpful\n",
    "- Model 2: Naive Bayes Bag of Words with Binary Helpfulness Labels\n",
    "- Model 3: TFIDF and Logistic Regression\n",
    "- Model 4: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t_9MV2_UO1O"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WXeTT4GwA9bN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import wget\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset - download directly from source, save to data directory\n",
    "\n",
    "file_name = \"data/reviews_Home_and_Kitchen_5.json.gz\"\n",
    "output_dir = \"data\"\n",
    "url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz\"\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    file_name = wget.download(url, out=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to parse data from compressed json into pandas DF\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def get_dataframe(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    \n",
    "    df_return = pd.DataFrame.from_dict(df, orient='index')\n",
    "    #shuffle all records to reduce bias of order of data\n",
    "    df_return = df_return.sample(frac=1)\n",
    "    \n",
    "    return df_return\n",
    "\n",
    "\n",
    "# helper function to pull out total helpful votes\n",
    "def get_helpful_votes(helpful):\n",
    "    [helpful, total] = helpful\n",
    "    return helpful\n",
    "\n",
    "\n",
    "# helper function to pull out total votes (helpful and unhelpful)\n",
    "def get_total_votes(helpful):\n",
    "    [helpful, total] = helpful\n",
    "    return total\n",
    "    \n",
    "    \n",
    "# helper function to calculate helpfulness percentage \n",
    "def calculate_helpful_perc(helpful):\n",
    "    [helpful, total] = helpful\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (helpful/total)\n",
    "    \n",
    "def balance_df(df):\n",
    "    df_non_one = df[df.helpful_perc != 1]\n",
    "    len_zero = len(df[df.helpful_perc == 0])\n",
    "    print(len_zero)\n",
    "    print(len(df[df.helpful_perc == 1]))\n",
    "    df_one = df[df.helpful_perc == 1].sample(len_zero)\n",
    "    df = df_non_one.append(df_one, ignore_index=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHEe5MIfBIsL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "23257\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "df = get_dataframe(file_name)\n",
    "\n",
    "# parse helpful column into new columns of helpful_votes, total_votes, helpful_perc\n",
    "df['helpful_votes'] = df['helpful'].apply(get_helpful_votes)\n",
    "df['total_votes'] = df['helpful'].apply(get_total_votes)\n",
    "df['helpful_perc'] = df['helpful'].apply(calculate_helpful_perc)\n",
    "\n",
    "\n",
    "# Now create the combinedText with casing and no longer than 512 characters (max of BERT embedding input)\n",
    "df['combinedText'] = df['summary'] + \". \" + df['reviewText']\n",
    "\n",
    "#drop rows where not enough votes exist to discern helpfullness\n",
    "kitchen_df = df.drop(df[df.total_votes < 5].index)\n",
    "balanced_kitchen_df = balance_df(kitchen_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCDusLkwUHJr"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551682, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 551,682 rows and twelve columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>helpful_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438202</th>\n",
       "      <td>A1UY3S4OR2ZLK4</td>\n",
       "      <td>B005DN65WG</td>\n",
       "      <td>John L</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>These mattress protector are terrific. Unlike ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Terrific Product</td>\n",
       "      <td>1391126400</td>\n",
       "      <td>01 31, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471318</th>\n",
       "      <td>A30WZ2LGA8VVPR</td>\n",
       "      <td>B0077L8YFI</td>\n",
       "      <td>Jonathan</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Pros: Really loved the look, it wasn't too dif...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Was disappointed so I returned it.  The weight...</td>\n",
       "      <td>1396310400</td>\n",
       "      <td>04 1, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68407</th>\n",
       "      <td>ATS3U1L4F89VQ</td>\n",
       "      <td>B0000DE9B8</td>\n",
       "      <td>Diane</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>You really can't go wrong with a glass measuri...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>You Can't Go Wrong</td>\n",
       "      <td>1124409600</td>\n",
       "      <td>08 19, 2005</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401258</th>\n",
       "      <td>A2JSKM60VTCNI</td>\n",
       "      <td>B004CLYAM2</td>\n",
       "      <td>Paul Stuart \"&amp;#34;...also I'll brush my teeth...</td>\n",
       "      <td>[8, 13]</td>\n",
       "      <td>Reviewer Disclaimer: This was received as a Re...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Crisp, clean air in a sleek package...but bett...</td>\n",
       "      <td>1341446400</td>\n",
       "      <td>07 5, 2012</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548589</th>\n",
       "      <td>A1JBBR4MNGQ70G</td>\n",
       "      <td>B00J7XVNAM</td>\n",
       "      <td>Falkor The White Luck Dragon</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>I needed a window air conditioner as the upsta...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Just What I Was Looking For</td>\n",
       "      <td>1402358400</td>\n",
       "      <td>06 10, 2014</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin  \\\n",
       "438202  A1UY3S4OR2ZLK4  B005DN65WG   \n",
       "471318  A30WZ2LGA8VVPR  B0077L8YFI   \n",
       "68407    ATS3U1L4F89VQ  B0000DE9B8   \n",
       "401258   A2JSKM60VTCNI  B004CLYAM2   \n",
       "548589  A1JBBR4MNGQ70G  B00J7XVNAM   \n",
       "\n",
       "                                            reviewerName  helpful  \\\n",
       "438202                                            John L   [0, 0]   \n",
       "471318                                          Jonathan   [0, 0]   \n",
       "68407                                              Diane   [2, 4]   \n",
       "401258  Paul Stuart \"&#34;...also I'll brush my teeth...  [8, 13]   \n",
       "548589                      Falkor The White Luck Dragon   [2, 2]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "438202  These mattress protector are terrific. Unlike ...      5.0   \n",
       "471318  Pros: Really loved the look, it wasn't too dif...      1.0   \n",
       "68407   You really can't go wrong with a glass measuri...      5.0   \n",
       "401258  Reviewer Disclaimer: This was received as a Re...      3.0   \n",
       "548589  I needed a window air conditioner as the upsta...      5.0   \n",
       "\n",
       "                                                  summary  unixReviewTime  \\\n",
       "438202                                   Terrific Product      1391126400   \n",
       "471318  Was disappointed so I returned it.  The weight...      1396310400   \n",
       "68407                                  You Can't Go Wrong      1124409600   \n",
       "401258  Crisp, clean air in a sleek package...but bett...      1341446400   \n",
       "548589                        Just What I Was Looking For      1402358400   \n",
       "\n",
       "         reviewTime  helpful_votes  total_votes  helpful_perc  \n",
       "438202  01 31, 2014              0            0      0.000000  \n",
       "471318   04 1, 2014              0            0      0.000000  \n",
       "68407   08 19, 2005              2            4      0.500000  \n",
       "401258   07 5, 2012              8           13      0.615385  \n",
       "548589  06 10, 2014              2            2      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twelve columns contain data including the reviewer name and review time, in addition to more applicable data to our models, like review text, summary, and helpful scores. The helpful column is a list of two numbers. The first number is the number of individuals who found that review helpful. The second number is the total number of individuals who scored that review.\n",
    "\n",
    "We parse out these two numbers in helpful_votes and total_votes. We calculate the helpfulness percentage as well in helpful_perc. This is simply the number of helpful_votes divided by the number of total_votes. Helpful_perc will be our target variable in our models.\n",
    "\n",
    "There are two sources of natural language in the dataset - reviewText and summary. We assume the reviewText will be more robust and impactful in determining whether a review is helpful or not. However, summary is a good resource for our models as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to examine the completeness of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4722e2898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAFECAYAAAAQt0QWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAH8lJREFUeJzt3Xm4ZVV95vHvWyAiKgiRoRVBICjaDFKCAqIySMAWeTQySGNUwCFKRxTFiE2i4JTYjcoQBxyQqCSIQwBHcKBERJnHqMEJjSQiBBpEEZBf/7HWqTp169YFyv1bZ91T7+d57lN1z4HzO/fWPu/ee42KCMzMbPIWTPoNmJlZ4UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw6seoD+Y/3WLBfs2l95957Bnss2K9VOddzvW5N++9yZagH6P78t75CNjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOrHqpN9AT756w5WTfgtmthJzII/Z81HbNKt17r3NSpnZPOFAHuMrZDObJAfyGF8hm9kkuVPPzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTDmQzs044kM3MOuFANjPrhAPZzKwTq076DfTkqzdcOem3YGYrMQfymD0ftU2zWufe26yUmc0TbrIwM+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBMOZDOzTjiQzcw64UA2M+uEA9nMrBcRkf4FvKJFnUnUm+afzfVcz/Xa1mt1hfyKRnUmUW+afzbXcz3Xa1jPTRZmZp1wIJuZdaJVIJ/cqM4k6k3zz+Z6rud6DeupNlSbmdmEucnCzKwTDmQzs044kM3MOpESyJJWlfRcSUfWr70lrZpRa1IkbSzpWfXvD5H08Em/p6FIOuf+PDZAnXXm+hq63ljd4yT996zXn6WeJL1I0t/W7zeS9JTkmhM5PiUtkLRm4us/VNKC+vfHSdpH0oOy6rU2eKeepEcD3wD+A7gcELAtsAGwa0TcMGjBUvNxwJHAxsDi4I+I3YauVeu9nDJAfJ2I2EzS5sAHI2L3geucCCz3HygiXjNwvdWA1YHzgZ0p/3YAawJfi4gtBq73U8rPp1mejojYdMh6Y3VfBhxMOVZOAf4pIv5fRq1a7wPAvcBuEfEESWsD50TE9kn1mhyfY/VOA/4S+ANwMeV4OT4i/k9CrUuBpwNrAxfUendFxEFD1xqreSpweETcWr9fGzguIg4ZulbGVes7gA9ExPvGH5T0GuBdwEsSap4BfBD4MOWgyHYY8BTgewARcZ2k9RLqXJLwmnM5DDgCWA+4liVBeRvl9zuoiNhk6Ne8n3U/AnxE0uMpwXyVpAuAD0fENxNKPjUiFkq6vNa/pZ78srQ6PkeeGBG3SToI+DLwJuBSYPBAplxE/lbSocD7I+Ldkq5IqDNu61EYw+J/v20zCmUE8g4R8dKZD0bECZJ+mFAP4J6I+EDSa8/m9xFxl1TyqjbHDD5+MCJOHfo176Pee4H3SnrtzBNqBkkL7+P9XJZYexVgi/p1E3AlcISkV0bECwcud3etF7X2upQr5ixNjs8xD6rNBs8DToqIuyVl1ZOkHYGDgEPrY6sk1RpZIGntiLilvoF1yMnOlBf93RzP/TahHsDZkl4NfB74/ejBiPivpHqLJL0ZeIikPYBXA2cn1ULSN5nlA5XVJAOsJWlBRNxb6z8MeG9EvHzgOsfN8VwAWU1O7wX2pjStvTMiLqpP/X3SRcMJlGNzPUnvAPYFjk6oM9L0+AQ+BPyMclL7lqSNKXdVGV4LHAV8PiKulbQpkHFXM+444EJJZ9Tv96O0BAwuow35J8AbZnsKeHdEbDZoQRa3Rc6U2Qa5gHJ2/jPKz/VV4CORNMtG0pPHvl0deAHlruCNSfXeDexCuZ1fH3g/pQ0y/ao5m8pl49HAeyLijlmeXyujPVnSFsDulOPl6xHx/aFrjNVqenwu5z2sGhH3JL7+GhGRdYE3W70nsuQC4RsR8a8pdRIC+ZS5no+IgwctuJKSdFFEpPXU1yurM4FbgV0i4t+yatV6WwJPpJxwAIiIf0yqdXVEbJXx2nPUXBt4DEt3Oqc1ybQkaX3gncCjIuLZNbx2jIiPJtTaEfgo8LCI2EjSNsArI+LVQ9eq9VYBrh26Q3u59ebz1GlJu0XENyT9+WzPR8TnkuruDbyNJaM6VMpFynCfGUPAFgBPBk6IiMcn1duJMmf/dGBL4KHAoRHxq6R6b6FckT8R+BLwbODbEbFvUr1TKW2dF2e8/iz13ga8FPgxS5qeInEUUOvj88uU0Sr/OyK2qW3Wl2ec9CR9j9Lkc1ZEbFsfuyYithy61ljNM4G/ioifZ9UYGbwNWdIRcz0fEe8ZsNwzKe2Az52tFJASyMD7gD8Hrm50G3gpS4aH3QP8lCUdGhlOAA6MiKsBJB0ALKJ0gGXYF9iG8iE+uF5xfTKpFsBTgYMkXQ/cwZLA2jqp3v7AZhFxV9Lrz9T6+HxkRHxa0lEAEXGPpLTRThHxi1GHZZU9smpt4FpJF1GOl9H72GfoQhmdes0mSETEW+qfrZtBfgFck32wS9ovIs4Ado+In2TWmmGH8fa/iDhd0jcS6/0uIu6VdE+dVHAj5fY+y56Jrz2ba4BHUH6uFpocn2PukPQnLBlFsgOQNa77F/UOLurIjsOBtPb46m+SX3+xlCaL2u7ymjqMKp2kwym3TLdTxiIvBN4UEYPPLqv1tqfcEi5i6VEdQ179I+myOn71soiYc4jYwHXXBd4ObBgRz6ltgk+JiI8n1Xs/8GbghcDrgd8AV2SfaOvY3PE265RbUknbUdrjr2Hp42XwK6xar8nxOVZvIXAipXnrGmBdYL+IuDKh1iOB44FnUe5szqFkTdaIqlHdjYHNI+JrktYAVomI2wevk3USze50mlHrytp2tSdlxtDRwCeyQkxlGvFvgKsZG08aEccMXOdcylXH9pTZc0tJ/EB/EfgU8Nf19/og4LKkNkFRgv8X9fvHAmtGxFVD1xqruQ9lKNOjKFetGwPfj4iU6dSSrqUMDZt5vCxKqtfk+Byr92BKs8HjKSH5Q2BBRPx+zv9xxWo9LSIuuK/HBq7ZbOZj5voSF0g6idIxNN7uktGzPGpQ+h/AP9bxibNNxx3KozI7EcY8h3K1/wnmHrM7tPUi4jRJRwLUgf4pExkiIiR9Cdiqfv+zjDozvA3YgTIdfFtJuwIvSqz324g4IfH1Z2p1fI5cWC9+rh09IOkyyrE7tBNned3ZHhtSs5mPmYH8pPrnsWOPZQ32v7ReFWwCHKWykErmTKgvSfqzrCaRkdoJ9F1JO0XEr2v7amTcKs1wRx3ZMWoT3J68gf4Al0navtWoB+DuiLhZZSGcBRHxTUmZY6zPl/Qu4CyWbkLIGvbW5PiUtAHwaMoElG1Zeu2TNQautSOwE7DujIEDa5I/U6/ZzMe0QI6IXbNeexaHUk4ADwK2Ax4JfDyx3quAN0j6PXA3ycOKgI1rp9rDKXf5twKHRMSlSfXeQJnZtamkRZQPXcoQtKr1qIdbVWYffgv4lKQbGbuLSzBa92CHscfSZiLS7vjckzKcb0NgvH36dkqfwJBWAx5GyazxgQO3kXtsQsOZj5ltyC0Hi7+M0tu6IXAF5cC/MGucZ2uSrgIOi4jz6/c7UxZWGTSwJO0QEd+tf18NeALlw/yvmUO2aofJMiLi+qR6DwXupPxsBwFrAZ+KiJsz6k07SS+IiM82qrVxRFxfT6hExG8a1Gw28zEzkFsOFr+a0vH13Yh4kso01XdGxKwTRgaquTawOUv30n8rqdblo0HwY48NPvKi9WiOGbV3pvRin1JHeTwsImabEj9kzTVZeubcoD31kl4UEZ9c3tj8hFE5W0TED7ScRZuymkgkPQL4W+AZ9aFFwLGRMwV9S0qfymiy1E3ASyLimqFrzai7GmUcfgA/zLpAyWxDbjlY/M6IuFMSkh5cD8qUWWyw/Cty8m5BF0n6EPBPlAPiAOC80QcvsS2yCZWZettReulPoTQ9fRJ4WlK9VwLHUK6S76Xe0gNDr30yakdtNTb/CMpogNk6gDObSD5KGe62f/3+Lyj/jhkXRCcDR0RdJlXSLvWxnRJqUWs8h7L87I8px8omKqsCfnnoWpmB3HKw+L/Xs/S/AOdKugVIud2tDmfJFfmuoyvyxHrb1D/fMuPxbRn2g7appLOW92TWMDvg+ZSf5bJa5wbl7nDxBmDLiLgpsQaUds+04Waz+Fit17L/BsosxBeMfX+M8tYofmiMrVkdEefVJqhMx1E21/gRgKTNgC9S1n4eVGYgv57Sq7yZyuLf65LU+B4Rz69/favKUpVrAV/JqFU1vSJv+AH7NW2H143cVYe/jU7e2R+wH5O3FOy4Q4CTGtQZeT+5w7+W53eSdo6Ib0MZF8zcy/D+MX4i6W8ozRZQhitmz2K9fRTGo/dA6bgcXOYoi0slPZOxweIRcXdWvbG6KYPtZ2hyRb68tseRhJlXtzf6/c306dok84g6CP8QyozLLEcB31FZqGZ8GNqgW2KtRF4FnCpprfr9LeTsDATl2DiGJevUfKs+lumSOlb+05Q70v2Ai1UXNYsBFzHL7NT7NqVx/3zgggZjZyeinnTWAr4ydEN/bVtdroSZgZ/L7Ai9j9p7MNaLHRHnJta6CPg2y85kG3SHFkn3MPuVeMowtDoccrkdy4kzO1eJiD/UTlIiIm3MuqSFrftMNPeSwhED7q2XGcibUDYjfDql0+v3wPkR8bqUgjaY1ifTeidwekT8MrPOWL1lRq3M5zpj9a4DXra85xOnav+c0kR4OmXx9rRFjWqT5AbAZyjHTOroivtD0lER8a5BXivxd4ek/0ZZIvPpwK7AzyNir7SCySTdzrK7JAel6We1iEhpAlLZVfsDwPoRsaWkrYF9IuLtSfWankzrncD+wH9RPtRnRNLay7XeOylbDp1N4pZfEwjkiQxbVFlsZ2/K4lALgS8A/zxqU06otwHleDmAMlPv9KzPwv18P4P93jOvkH9MGSN4GuVK64qoe7RNizo4/TDglZQ9vl6fVGcRcCTwoWi3KHfzk2k90RxA2aLq3yPiWUl1mmz5JenNEfFOSatHxJ1DvvZy6k2syWnsPaxNWY3toIhIndIsaSvgjcABEZG5i/d9vY/BTryZoyxOAHYGDqQMaVok6VsR8ePEmk3UDr3XAi+mnHC2T57ltUZEXKSl10vK3K9s/GT6UcpuCS1OpjcC/wncDKRtWx8Rm2S99ow6o6GQ10j6FeXC5HzKbiiDDwEdhfEk+m9qX8oBwF7AJSwZkzx0nSew5KR9M+WOKuVC6AEY7Ko2fQunehV5MGXs54bZZ81MKmuxvp5yQHwMODHjgzVL3S8D/4tyK79Q0r6ULZWenVTvcMrJ9DHADygf7rSTqcqO4ftThkaeAXw6kjaRrPVWoayk91iWnqmXsl5wrbkR5W7jaZRVCW+NiCfN/X+tcK3WTU4/Ay6njEI4K2bZPHbAWhcC/0z5LNyQVeeBmBdXyJKOo3yoHwZ8hzK1cpk1feeZ6yljdU+h9J4fOn7VmviBPowyG2kLSb+kbOF0UFItIuJ44Pixk+lbKbMSs06mj6FMtnkG5WrjQUl1Rs6mzNJbapRFFkkbUoL46ZRJPtdSRnmkiIifSroTuKt+7UpZlyTL1nONrBiy0ysidpzreUmfnTFJpYUzhnqhzDbkfSln5bTOmdYkvZUSGDM79oD0BcD3pVzRrUNZ4Soi4ti5/r8/ot7Mk+m3Kf+WKQPw6xX5yyhjS0WZuXdyRJyYVO+qyFtJbrZ69wIXU9ZXObNBva76b1p2Ng55tSrpROZojsgYt54ZyAuA/wlsEhFvq7dsG0TERSkFG1Hj7alqza8At1KmFi9eDyQiUmbVtT6Zqqxmt+PoVrfO1LswKzQl/T3w9UheL3is3jaUE9wzgI2A64BFkbDyYa3XtMnpfryfZqNNBh3xIM05uWXoceuQG8gfoNwO7hYRT6i9r+dExPYpBRtSw+2par3UERWz1Gt6MlVdrW80EkHS6sDFkbAyYH3951MWL1pAm/WsR30pO1OaLV5EKTjrsqMD15x4/03jK+SJrVg4hMxRFk+tHVCXA0TELSpL2E2DlttTQZnmu1VEXJ30+jP9A/VkStnu6Hbgs5QFlTKcAnxP0ufr98+jjO7I8h5gR+DqzEkMI5IuAR5Maf45H3hGJK31XOv11n+TuZ1aeq06GWWZ4yQS1lvPDOS76+39aMGYdWnQgdJIk+2p6pXjaOLJwZJ+Qukxz95Ro+nJNCLeI+k8SogAHBwRl2fVA34BXNMijKtnR8SvG9WCshTsuzvqvxms0+t++OuE13zD2N9Xpwy5Sxl2mtlkcRBleNhC4FRKp9TREdHyH2de03J20hjJuspSWXRnJ0qzwcJ6Mj2n5ayzTJI+Tln7+MssPVMvZZSMGu6eU+s1aXJq2ek1dnGyzFPkXpws7/2kNFtmrvb2KUmXArtTfmnPi4jvZ9VrqdUHLPO29j6cAHweWE/SO6gn0wm9lww/rV+r1a9sH6funlO//zdKc1dWs0yrJqdLBn69uezdsNZSVDb8HVkAPJmyoNjwtYa+Qpa0ZkTcNuOHWCwGXi9gEtRwe6pJUVl0f3Qy/fq0nEwnQdLFEbH9+GgDSVckTgy5bNTkNFbvyojY5r7+X1tWnWo/Gup6D+VkfmwkrNWRcYV8GuVsdilL32JkbZMzCS23p2pmxsn0RsqWUaPn1pmGkym07aSpWu6eA437b1r+Puvv7kTKRJfVKJOV7sgYISNpv9rEunvWGPyZBg/kiNi7/tlkvYAJaf0Ba2VlOJlCw06a6gga7Z5TtW5yavn7PImyqtwZlH0YXww8LqnWUbXOZ2i0E0tmp95ZlCusMyOixXY5zUh6MuWg35KyueO6wL4RcdVE35itsOyx5bVZq9nuOZNucsr6fUq6JCK2G59tmTXxRNK5lAuR7Zll2GAkLPifOeztOMooi7+TdDFlQZAvRINlCLPFhLanamWaT6YwayfNdiR00kjaLSK+obrVz5jHSRp0659abyJNTi07vYDf1iGYV0h6N/AftWaG51CujD9Bo70mW6z2tgqlt/flwF6Zs6Fa0ZRvT6UlSyk+h7IGw9ScTGGZTpq7KYvVD95JI+mYiHiLZt8CKGLArX9qvS9ExN5jP9/ip0hY73msbrNOrzoU9FeU9uPXUYL/HzKnhUtaNyJ+rbJFVWR+3rN3DHkI8FyWjEf+QkT8VVrBRrSSbE81jSdTAEn7U/ZAvE1lB+OFwNuyZlqq7jmX8dqTNOr0krRpq04vSYdHWY1wzscGrrkdZVTVwyknnVuBQyLi0qFrZV3qI+nTwPcpH+iTgM2mIYyhLG8InAt8nbKp5BrkLm/YXD2ZvgD4S0ob2uALqUzQ0TWMd6Ycnx+hbJGV5aeSTpa0u6T0acSSzpJ0oMrWSpmOqn9+JrnOuNkW/Hlpcs2PAa+OiMfW9UcOowT04DI79fYEvjalVwZdLW84tHoyfQpLNq5cNGU/3+URsa2kd1HWszgtq2Oo1mu951yTJqeWnV6SDqTMPtx5Rq01gT9ExO5D1Zql9jLHhpIWMcoM5DUow302iohXSNoceHxEfCGlYEPqbHnDoU3zyRRKWyvwS2APSkD+DrioxcQJtd1zLrXJqXaujTq9ltntOgbc5bq2HW8CvAt409hTtwNXRUTmlmbvAx5C6SQNysnuTsqKgYMuKpYZyKdTxrO+OMpOyWsA38manTQJ6mR5w6FN88kUFv98e1Gujq9T2dB1q0hcH1nL7jl3ekR8NrFes/6blp1etd76LJkGflFE3Jhc75tzPB1DToDJDOTReMGpm76pxjtqtLYynExbUsM952q9pk1OLTu9JO0H/F/gvFrr6cCREdGyHTtN5jjku+pZejSbbTPGVtaa53pb3nBom0XEAbXdjoj4bYvOqCk2555zCT4KHNiwyWnU6XU+QO0sPQXIWIHtaMpmBjfWWusCXyOhY1HSEXM9HwmrA6aMsqgf3g9SztCPkfQpyoiEN2bUm4DPAXvUIVNI2khSsx1EGpjmk+kkbCDp65KuAZC0taTMqcznA0dJOrnW21xS5mppfxiFMUDtrMxq010wo4niZvJGiz38Pr4Gl9lkcTWwC2WcroDvRsRNKcUa03RvTyXgL4BDgScC51B2TH5pRJw3wbc2b0laBBwJfGis+S5tW67WTU5NO73K7LxtWDIL8QBKp17GwvTNZTZZXAZsGhFfTKwxKVO7PVVEhKQjWfpkevi0nEwnZI2IuGhGq0/mYkatm5xG/UJvmfH4tgy/k04AH2LJ7jInU47TNJIeRxmnvn49wW0N7BMRbx+6VuqeesBBkq6n7Ds3kZX9k0zz9lQw3SfTSbipNvuMjpd9KWswZGna5BQRu2a99iz2qFfDi9cBkXQMOVs3jXyYeocDEBFXSToNmFeBvGfia0/atO+oMc0n00k4jHIlt4WkX1LWenhRRqHl9N88jYTZbC07vSS9Cng1sKmk8VUVHw5cMFSd5Wh2h5O5hdOkth9KF1O8PVU1zSfT5upwyGdJeiilUyptnG7jJqeUjq3lOI2yB+IyE0Mif+OEZnc46au9TROtBNtT2bBq09bao0CsfQ0vAY6IiJT1TySdCpwUERdnvP7KRtKmlDucnYBbKHc4B2VcdDqQHwBNaHlDm58kvZDS7ngHcB3wDsqY3YvJXV3uB8CfAk2anFp2ek2CpAdTmiUfC6wD3Eb5fR47eC0HslmOOu74eRHxI0kLKROK9o2Is5Prbjzb41nNiK2H9bUm6SuU2YeXAYsn20TE4IvWZ3bqTS1N+Y4aNpi7IuJHUMbiSrouO4xrrdb9N62H9bW2YUTs1aKQA3nFTO32VDao9WaMRHjE+PcZU28npPWwvta+I2mriLg6u5CbLP4I2csb2vwmaeZEiaVExDGt3kumlp1eLdXZxkG5cN0c+AllPHdam7wDeQW1XN7Qpo+k1SLirkm/jyG07PRqaXlt8SMZJxw3WayAGcsbnsSU7ahhw5J0HmUtkJ/V77enbBs175eirc5kSafXDRN+L4OZxBW+r5BXgKZ8Rw0bVj1ejqfM8Hw08GzgZVnD3lqbphEVk+ZAXgGa8h01bHiSdqFsjHsTsG1E/Odk39Fw6jKfJ7bo9Jp2brJYMadQljfcqX7/S+AMyuaVZkup62bvDzyDsmj7eZJeP98Xb5rR6XWwpPROr2nnQF4x3lHDHog/AZ4SEb8DLqwTDT4CzOtApuykbQNyIK8Y76hh91tEvHbG99dTdrye1+b7sLYeOZAfoJbLG9r8Jul9EfFaSWez9NonAETEPhN4W9Yxd+qtgGnensqGI+nJEXGppGfO9nxELGr9nqxvDuQV4OUNzSyDA3kFtF7e0OY3SU8D3gpsTGkm9HKtNisH8gpovbyhzW/1BP46ylDJ8eUbb57Ym7IuOZDNkkn6XkQ8ddLvw/rnQDZLJunvgFUoOyUvHh45LVOnbTgOZLNkkr5Z/zr6sI3akHeb0FuyTnkcslm+82Z5zFdCtgwHslm+34z9fXXKlOPvT+i9WMfcZGHWWF3Q/asRscuk34v1ZcGk34DZSmgNYMNJvwnrj5sszJKNLVMJZbTFusC83t7IcrjJwizZjIlE9wC/ioh7JvV+rF8OZDOzTrgN2cysEw5kM7NOOJDNzDrhQDYz64QD2cysE/8fXkuTv8Z2KngAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a46e8b9d30>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap shows that there are missing values for some observations of reviewerName. However, with such a large dataset, we could be missing some values for other features that just do not appear on this chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reviewerID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asin</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewerName</th>\n",
       "      <td>4953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpful</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewText</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unixReviewTime</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewTime</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpful_votes</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_votes</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helpful_perc</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "reviewerID         0\n",
       "asin               0\n",
       "reviewerName    4953\n",
       "helpful            0\n",
       "reviewText         0\n",
       "overall            0\n",
       "summary            0\n",
       "unixReviewTime     0\n",
       "reviewTime         0\n",
       "helpful_votes      0\n",
       "total_votes        0\n",
       "helpful_perc       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df = pd.DataFrame(df.isnull().sum())\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirm that we are only missing values for reviewerName. We reason that this does not have an impact on our analysis because we will be using primarily the reviewText, and possibly the summary, to determine a review's helpfulness. Additionally, we argue that individuals do not consistently use a reviewer's name when determining a review's helpfulness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing some sample data, and examining missing values, we next look at some summary statistics of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>helpful_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>551682.000000</td>\n",
       "      <td>5.516820e+05</td>\n",
       "      <td>551682.000000</td>\n",
       "      <td>551682.000000</td>\n",
       "      <td>551682.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.316655</td>\n",
       "      <td>1.348687e+09</td>\n",
       "      <td>3.497348</td>\n",
       "      <td>3.939469</td>\n",
       "      <td>0.367910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.110749</td>\n",
       "      <td>6.120238e+07</td>\n",
       "      <td>76.539142</td>\n",
       "      <td>77.801556</td>\n",
       "      <td>0.456931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.572256e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.331770e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.367626e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.388880e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.406074e+09</td>\n",
       "      <td>52176.000000</td>\n",
       "      <td>52861.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             overall  unixReviewTime  helpful_votes    total_votes  \\\n",
       "count  551682.000000    5.516820e+05  551682.000000  551682.000000   \n",
       "mean        4.316655    1.348687e+09       3.497348       3.939469   \n",
       "std         1.110749    6.120238e+07      76.539142      77.801556   \n",
       "min         1.000000    9.572256e+08       0.000000       0.000000   \n",
       "25%         4.000000    1.331770e+09       0.000000       0.000000   \n",
       "50%         5.000000    1.367626e+09       0.000000       0.000000   \n",
       "75%         5.000000    1.388880e+09       1.000000       2.000000   \n",
       "max         5.000000    1.406074e+09   52176.000000   52861.000000   \n",
       "\n",
       "        helpful_perc  \n",
       "count  551682.000000  \n",
       "mean        0.367910  \n",
       "std         0.456931  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These summary statistics show that over half of our observations have a 0% helpfulness. In addition, these observations have zero total votes. This means that the reviews simply haven't been voted upon. We should remove these observations from our dataset because our model could misinterpret the 0% helpful_perc to mean that the review was not helpful when in fact the review just hasn't been voted upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a histogram to visualize the distribution of helpful percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a47c27a240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a472fc0ac8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax1.hist(x=df['helpful_perc'], bins='auto')\n",
    "                                 \n",
    "ax1.set_xlabel('Helpful Perc')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Histogram of Helpful Percentages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows that the majority of reviews have a 0% helpful percentage, with the second most prevalent percentage being 100%. However, our dataset needs to be cleaned to remove observations that may add noise, but do not add meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be purposeful with the data that we are supplying to our model. We want to only analyze reviews where there are at least three total votes, or reviews where there are two total votes, but both votes are in agreement.\n",
    "\n",
    "Our rationale is that two total votes that are split does not tell us much. However, if both are an agreeement, that could tell us something about a reviews helpfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139470, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset df of reviews that have at least three votes\n",
    "df_three = df[df.total_votes >= 3]\n",
    "\n",
    "# subset df of reviews that have two both and both are in agreeement\n",
    "df_two = df[((df.total_votes == 2) & (df.helpful_perc == 1)) | ((df.total_votes == 2) & (df.helpful_perc == 0))]\n",
    "\n",
    "# combine the dfs back together\n",
    "df = df_three.append(df_two, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of our dataset is now 139,470 rows. Next we can examine the new distribution of helpful_perc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>helpful_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>139470.000000</td>\n",
       "      <td>1.394700e+05</td>\n",
       "      <td>139470.000000</td>\n",
       "      <td>139470.000000</td>\n",
       "      <td>139470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.988270</td>\n",
       "      <td>1.301792e+09</td>\n",
       "      <td>13.178676</td>\n",
       "      <td>14.704022</td>\n",
       "      <td>0.848035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.356746</td>\n",
       "      <td>8.137408e+07</td>\n",
       "      <td>151.811710</td>\n",
       "      <td>154.232187</td>\n",
       "      <td>0.242654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.572256e+08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.261181e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.324080e+09</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.362874e+09</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.405728e+09</td>\n",
       "      <td>52176.000000</td>\n",
       "      <td>52861.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             overall  unixReviewTime  helpful_votes    total_votes  \\\n",
       "count  139470.000000    1.394700e+05  139470.000000  139470.000000   \n",
       "mean        3.988270    1.301792e+09      13.178676      14.704022   \n",
       "std         1.356746    8.137408e+07     151.811710     154.232187   \n",
       "min         1.000000    9.572256e+08       0.000000       2.000000   \n",
       "25%         3.000000    1.261181e+09       2.000000       3.000000   \n",
       "50%         5.000000    1.324080e+09       4.000000       5.000000   \n",
       "75%         5.000000    1.362874e+09       9.000000      10.000000   \n",
       "max         5.000000    1.405728e+09   52176.000000   52861.000000   \n",
       "\n",
       "        helpful_perc  \n",
       "count  139470.000000  \n",
       "mean        0.848035  \n",
       "std         0.242654  \n",
       "min         0.000000  \n",
       "25%         0.750000  \n",
       "50%         1.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median helpful_perc is 100%. We could be at risk of not having a good distribution of data to train on. This could result in our model being overly optimistic and inflating the helpfulness scores of reviews. We can visualize this skew with another histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a417e89198>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a419904828>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax2.hist(x=df['helpful_perc'], bins='auto')\n",
    "                                 \n",
    "ax2.set_xlabel('Helpful Perc')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Histogram of Helpful Percentages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to resource constraints, we'll subset our data to roughly half the original size. This is still more than enough data to build and test our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled = df.sample(80000)\n",
    "df_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1a40e73e0f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41546ad30>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig3 = plt.figure()\n",
    "ax3 = fig3.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax3.hist(x=df['helpful_perc'], bins='auto')\n",
    "                                 \n",
    "ax3.set_xlabel('Helpful Perc')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Histogram of Helpful Percentages of Subsetted Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the sampled data visually matches the distribution of the original dataset. We can confirm the distributions are identical with the Kolmogorov-Smirnov statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ks_2sampResult(statistic=0.0018642270739226974, pvalue=0.9944665853896963)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "df_sampled_helpful_prec = df_sampled['helpful_perc']\n",
    "df_helpful_perc = df['helpful_perc']\n",
    "\n",
    "ks_2samp(df_sampled_helpful_prec, df_helpful_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_sampled becomes our df\n",
    "df = df_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Every Review is Helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic model we can produce is to label each review as being helpful if it meets a certain helpful_perc threshold. We choose 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every Review is Helpful Accuracy Score ->  79.21125\n"
     ]
    }
   ],
   "source": [
    "# calculate total number of reviews\n",
    "total_reviews = len(df)\n",
    "\n",
    "# calculate total number of reviews that are at least 75% helpful\n",
    "helpful_reviews = len(df[df.helpful_perc >= 0.75])\n",
    "\n",
    "# calculate accuracy if we predicted each review is at least 75% helpful\n",
    "accuracy_1 = 100*(helpful_reviews/total_reviews)\n",
    "\n",
    "print(\"Every Review is Helpful Accuracy Score -> \", accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would produce an accuracy score of 79.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Naive Bayes Bag of Words with Binary Helpfulness Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated model is one where we estimate the hepfulness of a review using its text. We use a Bag of Words model to determine whether a review is helpful or not. We again define a helpful review as one whose helpfulness percentage is at least 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# this nltk download may be needed. download stopwords, punkt\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure there are no missing values in the review or summary text or the overall helpullness percentage\n",
    "len(df) == len(df.dropna(subset=['reviewText', 'summary', 'helpful_perc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our bag of words model, we will combine the reviewText and summary text fields to combine into a single \"document\" with which to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    return [word for word in word_tokenize(text) if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We should then tokenize and stem the review data before ingesting into our NLP models\n",
    "df['combinedText'] = df['summary'].str.lower() + \". \" +  df['reviewText'].str.lower()\n",
    "\n",
    "# Now tokenize these and remove stop words\n",
    "df['processedText'] = df['combinedText'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new column for helpful reviews (reviews with at least a 75% helpfulness rating)\n",
    "\n",
    "df[\"isHelpful\"] = df[\"helpful_perc\"] > .75\n",
    "df[\"isHelpful\"] = df[\"isHelpful\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>helpful_perc</th>\n",
       "      <th>combinedText</th>\n",
       "      <th>processedText</th>\n",
       "      <th>isHelpful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106615</th>\n",
       "      <td>A1FNLY02ZH11MW</td>\n",
       "      <td>B00E58P7ME</td>\n",
       "      <td>J. VanGoethem</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>Yes, it costs too much for a $.25 worth of met...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Does the job</td>\n",
       "      <td>1315699200</td>\n",
       "      <td>09 11, 2011</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>does the job. yes, it costs too much for a $.2...</td>\n",
       "      <td>[job, ., yes, ,, costs, much, $, .25, worth, m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61951</th>\n",
       "      <td>A22MANL4US4RMY</td>\n",
       "      <td>B001V4VMRO</td>\n",
       "      <td>Book Carpenter</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>This cover is a nice, tight fit, and it seems ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Good Fit, Good Construction</td>\n",
       "      <td>1314144000</td>\n",
       "      <td>08 24, 2011</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>good fit, good construction. this cover is a n...</td>\n",
       "      <td>[good, fit, ,, good, construction, ., cover, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119108</th>\n",
       "      <td>A3NT7RAITJZXXT</td>\n",
       "      <td>B000GX4CKA</td>\n",
       "      <td>Linda \"cooking enthusiast\"</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>While this is a good shelf for the price it wo...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>No instructions included</td>\n",
       "      <td>1279929600</td>\n",
       "      <td>07 24, 2010</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>no instructions included. while this is a good...</td>\n",
       "      <td>[instructions, included, ., good, shelf, price...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15628</th>\n",
       "      <td>A1VB2KE2PR4J4W</td>\n",
       "      <td>B0000CFNQX</td>\n",
       "      <td>J. J. Emmel</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>It's made in China, a tad small, but not bad f...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>not bad for made in China</td>\n",
       "      <td>1329177600</td>\n",
       "      <td>02 14, 2012</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>not bad for made in china. it's made in china,...</td>\n",
       "      <td>[bad, made, china, ., 's, made, china, ,, tad,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99353</th>\n",
       "      <td>A2QFO64UAU5N18</td>\n",
       "      <td>B008R3ER0Q</td>\n",
       "      <td>W. Lucas \"wvl\"</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>This thing is very weak on the suction. I read...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Must have gotten a bad one, because</td>\n",
       "      <td>1388361600</td>\n",
       "      <td>12 30, 2013</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>must have gotten a bad one, because. this thin...</td>\n",
       "      <td>[must, gotten, bad, one, ,, ., thing, weak, su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID        asin                reviewerName helpful  \\\n",
       "106615  A1FNLY02ZH11MW  B00E58P7ME               J. VanGoethem  [3, 4]   \n",
       "61951   A22MANL4US4RMY  B001V4VMRO              Book Carpenter  [2, 3]   \n",
       "119108  A3NT7RAITJZXXT  B000GX4CKA  Linda \"cooking enthusiast\"  [2, 2]   \n",
       "15628   A1VB2KE2PR4J4W  B0000CFNQX                 J. J. Emmel  [3, 4]   \n",
       "99353   A2QFO64UAU5N18  B008R3ER0Q              W. Lucas \"wvl\"  [2, 4]   \n",
       "\n",
       "                                               reviewText  overall  \\\n",
       "106615  Yes, it costs too much for a $.25 worth of met...      4.0   \n",
       "61951   This cover is a nice, tight fit, and it seems ...      5.0   \n",
       "119108  While this is a good shelf for the price it wo...      3.0   \n",
       "15628   It's made in China, a tad small, but not bad f...      3.0   \n",
       "99353   This thing is very weak on the suction. I read...      1.0   \n",
       "\n",
       "                                    summary  unixReviewTime   reviewTime  \\\n",
       "106615                         Does the job      1315699200  09 11, 2011   \n",
       "61951           Good Fit, Good Construction      1314144000  08 24, 2011   \n",
       "119108             No instructions included      1279929600  07 24, 2010   \n",
       "15628             not bad for made in China      1329177600  02 14, 2012   \n",
       "99353   Must have gotten a bad one, because      1388361600  12 30, 2013   \n",
       "\n",
       "        helpful_votes  total_votes  helpful_perc  \\\n",
       "106615              3            4      0.750000   \n",
       "61951               2            3      0.666667   \n",
       "119108              2            2      1.000000   \n",
       "15628               3            4      0.750000   \n",
       "99353               2            4      0.500000   \n",
       "\n",
       "                                             combinedText  \\\n",
       "106615  does the job. yes, it costs too much for a $.2...   \n",
       "61951   good fit, good construction. this cover is a n...   \n",
       "119108  no instructions included. while this is a good...   \n",
       "15628   not bad for made in china. it's made in china,...   \n",
       "99353   must have gotten a bad one, because. this thin...   \n",
       "\n",
       "                                            processedText  isHelpful  \n",
       "106615  [job, ., yes, ,, costs, much, $, .25, worth, m...          0  \n",
       "61951   [good, fit, ,, good, construction, ., cover, n...          0  \n",
       "119108  [instructions, included, ., good, shelf, price...          1  \n",
       "15628   [bad, made, china, ., 's, made, china, ,, tad,...          0  \n",
       "99353   [must, gotten, bad, one, ,, ., thing, weak, su...          0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's make sure everything looks ok\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of Helpful/Not Helpful Reviews')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAETCAYAAADKy1riAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCQVMzQkScUMDUlSMUNDklTM0JAkFfv/S/1T7Rt/GsAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take a look at distribution of Helpful/Not Helpful\n",
    "\n",
    "fig4 = plt.figure()\n",
    "ax4 = fig4.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax4.hist(x=df['isHelpful'], bins=2)\n",
    "                                 \n",
    "ax4.set_xlabel('Helpful')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Histogram of Helpful/Not Helpful Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = df[['combinedText', 'isHelpful']]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "preprocessed_data = data.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-33ae99ce0175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m Train_X, Test_X, Train_Y, Test_Y = train_test_split(preprocessed_data['combinedText'], \n\u001b[0m\u001b[0;32m      4\u001b[0m                                                     \u001b[0mpreprocessed_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'isHelpful'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                     test_size=0.2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_X, Test_X, Train_Y, Test_Y = train_test_split(preprocessed_data['combinedText'], \n",
    "                                                    preprocessed_data['isHelpful'],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Accuracy Score ->  74.76875\n"
     ]
    }
   ],
   "source": [
    "# and train our classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "g_classifier = GaussianNB().fit(np.array(Train_X.values.tolist()).reshape(-1, 1), Train_Y)\n",
    "accuracy_2 = accuracy_score(Test_Y, g_classifier.predict(np.array(Test_X.values.tolist()).reshape(-1, 1)))*100\n",
    "\n",
    "print(\"Gaussian Naive Bayes Accuracy Score -> \", accuracy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Naive Bayes Bag of Words model performs worse than our previous \"every review is helpful\" baseline model. It produces an accuracy score of 75.3%, down from 79.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sklearn\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train/validation/test sets\n",
    "data = df[['combinedText', 'isHelpful']]\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size = 0.2)\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stem combinedText to reduce size of corpus\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stem_text(df):\n",
    "    text_list = df['combinedText'].tolist()\n",
    "    text_list_stem = [None] * len(text_list)\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        text_list_stem[i] = ' '.join([porter.stem(w) for w in text_list[i].split()])\n",
    "    \n",
    "    return text_list_stem\n",
    "\n",
    "text_list_stem = stem_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03389221, 0.07340804, 0.03054271, ..., 0.03196743, 0.099017  ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.19454242, 0.        , 0.        , ..., 0.        , 0.14209024,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.26798256,\n",
       "        0.        ],\n",
       "       [0.03311168, 0.07171747, 0.08951795, ..., 0.        , 0.145105  ,\n",
       "        0.14716066],\n",
       "       [0.        , 0.        , 0.18267261, ..., 0.        , 0.14805248,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                max_features=100,\n",
    "                ngram_range=(1,1)\n",
    "                )\n",
    "\n",
    "vectorizer.fit_transform(text_list_stem).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (51200, 100)\n"
     ]
    }
   ],
   "source": [
    "# vectorize train dataset\n",
    "train_text_stem = stem_text(df_train)\n",
    "train_vectorized = vectorizer.transform(train_text_stem).toarray()\n",
    "\n",
    "print('Shape:', train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (12800, 100)\n"
     ]
    }
   ],
   "source": [
    "# vectorize val dataset\n",
    "val_text_stem = stem_text(df_val)\n",
    "val_vectorized = vectorizer.transform(val_text_stem).toarray()\n",
    "\n",
    "print('Shape:', val_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (16000, 100)\n"
     ]
    }
   ],
   "source": [
    "# vectorize test dataset\n",
    "test_text_stem = stem_text(df_test)\n",
    "test_vectorized = vectorizer.transform(test_text_stem).toarray()\n",
    "\n",
    "print('Shape:', test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create array of labels to use in logistic regression\n",
    "df_train_labels = np.array(df_train['isHelpful'])\n",
    "df_test_labels = np.array(df_test['isHelpful'])\n",
    "df_val_labels = np.array(df_val['isHelpful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train logistic regression\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "lr.fit(train_vectorized, df_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strength: 0.01   validation set accuracy: 0.745546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strength: 0.1   validation set accuracy: 0.746796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strength: 1.0   validation set accuracy: 0.747265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strength: 10   validation set accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strength: 100   validation set accuracy: 0.74765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strength: 1000   validation set accuracy: 0.74765625\n"
     ]
    }
   ],
   "source": [
    "# tune hyperparameters\n",
    "reg_str = [0.01, 0.1, 1.0, 10, 100, 1000]\n",
    "\n",
    "best_acc = -1\n",
    "best_c = None\n",
    "\n",
    "for r in reg_str:\n",
    "    lr = LogisticRegression(penalty='l2', C=r)\n",
    "    lr.fit(train_vectorized, df_train_labels)\n",
    "    preds = lr.predict(val_vectorized)\n",
    "    acc = np.mean(preds == df_val_labels)\n",
    "    print('regularization strength:', r, ' ', 'validation set accuracy:', acc)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_c = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF and Logistic Regression Accuracy Score -> 74.99375\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "lr_final = LogisticRegression(penalty='l2', C = best_c)\n",
    "lr_final.fit(train_vectorized, df_train_labels)\n",
    "\n",
    "preds = lr_final.predict(test_vectorized)\n",
    "accuracy_3 = 100*np.mean(preds == df_test_labels)\n",
    "\n",
    "print(\"TF-IDF and Logistic Regression Accuracy Score ->\", accuracy_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with term frequencyinverse document frequency and logistic regression produces a marginally higher accuracy than the Naive Bayes Bag of Words model, but it is still not higher than the rudimentary Every Review is Helpful model. This motivates the need for a more robust representation of the language in the reviews. Enter BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code built around Strongio's notebook here https://github.com/strongio/keras-bert/blob/master/keras-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# if this doesnt work, ensure tensorflow is version <2.0\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Tensorflow hub path to BERT module of choice\n",
    "BERT_DIR =  \"./bert\"\n",
    "BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "sys.path.insert(0,BERT_DIR)\n",
    "\n",
    "# Initialize session, may need to remove config if no GPU\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.text_a)\n",
    "\n",
    "def create_tokenizer_from_hub_module(path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256, tokenb=True):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if tokenb:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "    else:\n",
    "        tokens_b = []\n",
    "    \n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    elif (len(tokens_a)+len(tokens_b)) > max_seq_length - 3:\n",
    "        tokens_b = tokens_b[0 : (max_seq_length - 3 - len(tokens_a))]\n",
    "        \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    \n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokenb:\n",
    "        if len(tokens) < max_seq_length:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length, tokenb=False\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples2(titles, labels, texts=None, max_examples=None):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for title, label in zip(titles, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=title, text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples[:max_examples]\n",
    "\n",
    "def convert_text_to_examples(titles, texts, labels, max_examples=None):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for title, text, label in zip(titles, texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=title, text_b=text, label=label)\n",
    "        )\n",
    "    return InputExamples[:max_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module(BERT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "23257\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "df = get_dataframe(file_name)\n",
    "\n",
    "# parse helpful column into new columns of helpful_votes, total_votes, helpful_perc\n",
    "df['helpful_votes'] = df['helpful'].apply(get_helpful_votes)\n",
    "df['total_votes'] = df['helpful'].apply(get_total_votes)\n",
    "df['helpful_perc'] = df['helpful'].apply(calculate_helpful_perc)\n",
    "\n",
    "\n",
    "# Now create the combinedText with casing and no longer than 512 characters (max of BERT embedding input)\n",
    "df['combinedText'] = df['summary'] + \". \" + df['reviewText']\n",
    "\n",
    "#drop rows where not enough votes exist to discern helpfullness\n",
    "kitchen_df = df.drop(df[df.total_votes < 5].index)\n",
    "balanced_kitchen_df = balance_df(kitchen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(balanced_kitchen_df, test_size = 0.2)\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:105: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5ba1f6f0fd48d494b87719044e9c2e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd9cd7dbba44b108f41954d8fc37d24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6867521fb3ac4a068d39ae0f6be7f3b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data to InputExample format, ensure we don't have too many examples to encounter memory issues\n",
    "train_examples = convert_text_to_examples(df_train['summary'], df_train['reviewText'], df_train['helpful_perc'], max_examples=1000)\n",
    "val_examples = convert_text_to_examples(df_val['summary'], df_val['reviewText'], df_val['helpful_perc'], max_examples=200)\n",
    "test_examples = convert_text_to_examples(df_test['summary'], df_test['reviewText'], df_test['helpful_perc'], max_examples=200)\n",
    "\n",
    "#make sure the test Y is the same format\n",
    "test_actual = np.array(df_test['helpful_perc'][:200])\n",
    "test_actual = test_actual.reshape(-1,1)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQ_LENGTH)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels \n",
    ") = convert_examples_to_features(tokenizer, val_examples, max_seq_length=MAX_SEQ_LENGTH)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>helpful_perc</th>\n",
       "      <th>combinedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28393</th>\n",
       "      <td>A1JJOV69MAU2J2</td>\n",
       "      <td>B00004OCK0</td>\n",
       "      <td>Steven Dennis</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>These tongs are cleverly designed and solidly ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The best pair of tongs at a bargain price...</td>\n",
       "      <td>999820800</td>\n",
       "      <td>09 7, 2001</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>The best pair of tongs at a bargain price.... ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27884</th>\n",
       "      <td>A1MAKVM3ATO1DC</td>\n",
       "      <td>B008BDSBEU</td>\n",
       "      <td>Kaylie</td>\n",
       "      <td>[25, 26]</td>\n",
       "      <td>I was skeptical to buy this because I was worr...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Beautiful!!!!</td>\n",
       "      <td>1358985600</td>\n",
       "      <td>01 24, 2013</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>Beautiful!!!!. I was skeptical to buy this bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33598</th>\n",
       "      <td>A18H3X0XGMD1K0</td>\n",
       "      <td>B007D3V00Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>I have not used this press because it will be ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Will be difficult to clean</td>\n",
       "      <td>1369785600</td>\n",
       "      <td>05 29, 2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Will be difficult to clean. I have not used th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43366</th>\n",
       "      <td>A14CPE16HGLDW2</td>\n",
       "      <td>B005SI8YZC</td>\n",
       "      <td>Sam Mercer \"Sam\"</td>\n",
       "      <td>[6, 8]</td>\n",
       "      <td>Steam blown.  Wow, this thing really gets hot....</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My mind is blown</td>\n",
       "      <td>1355788800</td>\n",
       "      <td>12 18, 2012</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>My mind is blown. Steam blown.  Wow, this thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>A2CGJON31CMTCR</td>\n",
       "      <td>B0049SHD7C</td>\n",
       "      <td>silverorchid</td>\n",
       "      <td>[6, 9]</td>\n",
       "      <td>I have had several waffle makers. Since most o...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Average product with few flaws</td>\n",
       "      <td>1364428800</td>\n",
       "      <td>03 28, 2013</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Average product with few flaws. I have had sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12726</th>\n",
       "      <td>A15FA4XWUDLMI7</td>\n",
       "      <td>B0000CFOIH</td>\n",
       "      <td>W. Peterson \"creative cook\"</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>I really like using this for those special lit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Nice little tool for the kitchen</td>\n",
       "      <td>1233100800</td>\n",
       "      <td>01 28, 2009</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Nice little tool for the kitchen. I really lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26378</th>\n",
       "      <td>A1TS45JWJVOSSW</td>\n",
       "      <td>B0016LQ01K</td>\n",
       "      <td>Duane Sparks \"Duane\"</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>Cooks and cleans perfectly, and has already be...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfect product</td>\n",
       "      <td>1314230400</td>\n",
       "      <td>08 25, 2011</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>Perfect product. Cooks and cleans perfectly, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>A1Z54EM24Y40LL</td>\n",
       "      <td>B00004SGFS</td>\n",
       "      <td>csm</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>If you're bored some day, buy these attachment...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome attachment!</td>\n",
       "      <td>1104451200</td>\n",
       "      <td>12 31, 2004</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Awesome attachment!. If you're bored some day,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23521</th>\n",
       "      <td>A3Q20W5LJTGC7T</td>\n",
       "      <td>B00164W8HM</td>\n",
       "      <td>swrobel</td>\n",
       "      <td>[14, 18]</td>\n",
       "      <td>Well, I think for once the old adage \"you get ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>You get more than what you pay for</td>\n",
       "      <td>1243900800</td>\n",
       "      <td>06 2, 2009</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>You get more than what you pay for. Well, I th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9833</th>\n",
       "      <td>A3HNW0MW20ZIXF</td>\n",
       "      <td>B0001KOA4Q</td>\n",
       "      <td>Dominika Lepak \"nikatoo\"</td>\n",
       "      <td>[104, 116]</td>\n",
       "      <td>For the price, I'd give the espresso making pa...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great espresso, poor steaming wand</td>\n",
       "      <td>1111968000</td>\n",
       "      <td>03 28, 2005</td>\n",
       "      <td>104</td>\n",
       "      <td>116</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>Great espresso, poor steaming wand. For the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30174</th>\n",
       "      <td>A2JP0URFHXP6DO</td>\n",
       "      <td>B0000E19MW</td>\n",
       "      <td>Tim Janson</td>\n",
       "      <td>[297, 311]</td>\n",
       "      <td>I've prided myself on my homemade pizza for ye...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>THE BEST HOME PIZZA YET!</td>\n",
       "      <td>1108598400</td>\n",
       "      <td>02 17, 2005</td>\n",
       "      <td>297</td>\n",
       "      <td>311</td>\n",
       "      <td>0.954984</td>\n",
       "      <td>THE BEST HOME PIZZA YET!. I've prided myself o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6910</th>\n",
       "      <td>A1BZN88T4GSZG0</td>\n",
       "      <td>B00DBTOVXO</td>\n",
       "      <td>Silly Sister</td>\n",
       "      <td>[10, 11]</td>\n",
       "      <td>Overall I'm pleased with the Bissell Carpet Cl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Worth the price!</td>\n",
       "      <td>1384387200</td>\n",
       "      <td>11 14, 2013</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>Worth the price!. Overall I'm pleased with the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>A2R5R53I1LMUDG</td>\n",
       "      <td>B001HI3YOS</td>\n",
       "      <td>amazon addicted</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>These jars are really nice for the price. They...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Nice Jars for the price!</td>\n",
       "      <td>1349136000</td>\n",
       "      <td>10 2, 2012</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Nice Jars for the price!. These jars are reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18167</th>\n",
       "      <td>A2NJ6M5VQYEVHC</td>\n",
       "      <td>B0000BYC4B</td>\n",
       "      <td>T. F.</td>\n",
       "      <td>[103, 114]</td>\n",
       "      <td>According to the manufacturer's web site, Ball...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Does not contain bisphenol A (BPA)</td>\n",
       "      <td>1314576000</td>\n",
       "      <td>08 29, 2011</td>\n",
       "      <td>103</td>\n",
       "      <td>114</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>Does not contain bisphenol A (BPA). According ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26499</th>\n",
       "      <td>A2TDQ6U4FXYTNP</td>\n",
       "      <td>B000FZZ0VE</td>\n",
       "      <td>Maeve \"Maeve\"</td>\n",
       "      <td>[8, 27]</td>\n",
       "      <td>This breadmaker makes good bread, but plan to ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good bread, but...</td>\n",
       "      <td>1170806400</td>\n",
       "      <td>02 7, 2007</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>Good bread, but.... This breadmaker makes good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42864</th>\n",
       "      <td>A5BCJVGF1LA4O</td>\n",
       "      <td>B000G0OJ9C</td>\n",
       "      <td>Likes to Shop</td>\n",
       "      <td>[1, 20]</td>\n",
       "      <td>This is a great little heater thats really pow...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great Heater!</td>\n",
       "      <td>1163721600</td>\n",
       "      <td>11 17, 2006</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>Great Heater!. This is a great little heater t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11422</th>\n",
       "      <td>A3V6Z4RCDGRC44</td>\n",
       "      <td>B000COC5MK</td>\n",
       "      <td>Lisa Shea \"be the change you wish to see in t...</td>\n",
       "      <td>[170, 186]</td>\n",
       "      <td>I really had high hopes for the Oster. We open...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Nice Idea - Poor Long Term Usefulness</td>\n",
       "      <td>1222992000</td>\n",
       "      <td>10 3, 2008</td>\n",
       "      <td>170</td>\n",
       "      <td>186</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>Nice Idea - Poor Long Term Usefulness. I reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30848</th>\n",
       "      <td>A3CBKR4HL03YR3</td>\n",
       "      <td>B0033XX5H6</td>\n",
       "      <td>Sassy Lady</td>\n",
       "      <td>[21, 22]</td>\n",
       "      <td>I was thrilled to have found this set at such ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great Price on Great Product</td>\n",
       "      <td>1278028800</td>\n",
       "      <td>07 2, 2010</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>Great Price on Great Product. I was thrilled t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27525</th>\n",
       "      <td>A14H5GNH2EIPAC</td>\n",
       "      <td>B007M0HNYC</td>\n",
       "      <td>Kuromame's Mommy \"Rose\"</td>\n",
       "      <td>[1, 5]</td>\n",
       "      <td>I got this as a freebie with another order.  I...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>digital kitchen scale</td>\n",
       "      <td>1347840000</td>\n",
       "      <td>09 17, 2012</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>digital kitchen scale. I got this as a freebie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33239</th>\n",
       "      <td>A23A3FEXAMBOIJ</td>\n",
       "      <td>B00005Q5K4</td>\n",
       "      <td>fairview</td>\n",
       "      <td>[6, 8]</td>\n",
       "      <td>This teakettle was given to me as a gift. I wi...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Disappointed with this teapot</td>\n",
       "      <td>1116892800</td>\n",
       "      <td>05 24, 2005</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Disappointed with this teapot. This teakettle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35259</th>\n",
       "      <td>A1PL0PBCQ6YC9E</td>\n",
       "      <td>B0006IQ2NA</td>\n",
       "      <td>Victor S. \"vs.\"</td>\n",
       "      <td>[19, 20]</td>\n",
       "      <td>This is an OK item. I did not want to have 3 s...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10 inch - what it stands for?</td>\n",
       "      <td>1305504000</td>\n",
       "      <td>05 16, 2011</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>10 inch - what it stands for?. This is an OK i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>A2EFG35K3TA4VZ</td>\n",
       "      <td>B0000Z6JK0</td>\n",
       "      <td>T. Dietz</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>I bought this thing thinking believing others ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>overheats.  worthless for anything but chicken...</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>03 13, 2012</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>overheats.  worthless for anything but chicken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35741</th>\n",
       "      <td>A3B62CEEBAKM7M</td>\n",
       "      <td>B001EX44C2</td>\n",
       "      <td>USA</td>\n",
       "      <td>[19, 21]</td>\n",
       "      <td>I had high hopes for this after reading all th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I had high hopes for this</td>\n",
       "      <td>1275782400</td>\n",
       "      <td>06 6, 2010</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>I had high hopes for this. I had high hopes fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33778</th>\n",
       "      <td>AQZ1WKIJT3ZTT</td>\n",
       "      <td>B0007MS5B2</td>\n",
       "      <td>TheBigOldDog</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>I purchased 2 of these when  they first came o...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Save your money and buy a Scooba instead</td>\n",
       "      <td>1201219200</td>\n",
       "      <td>01 25, 2008</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Save your money and buy a Scooba instead. I pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45978</th>\n",
       "      <td>A2IO1ESNSIAXG3</td>\n",
       "      <td>B00004S1B8</td>\n",
       "      <td>L. A. Kane</td>\n",
       "      <td>[46, 47]</td>\n",
       "      <td>Nothing beats a good whetstone, but you need t...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Nothing beats a good whetstone but...</td>\n",
       "      <td>1145145600</td>\n",
       "      <td>04 16, 2006</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>Nothing beats a good whetstone but.... Nothing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8835</th>\n",
       "      <td>A1G4BESDDK9679</td>\n",
       "      <td>B0084DAI3O</td>\n",
       "      <td>LesfromSanDiego \"Les\"</td>\n",
       "      <td>[21, 25]</td>\n",
       "      <td>We really like the air circulation with this V...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great Fan. Does exactly what it claims to do</td>\n",
       "      <td>1369612800</td>\n",
       "      <td>05 27, 2013</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>Great Fan. Does exactly what it claims to do. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31759</th>\n",
       "      <td>ALNAXN9SFT7BB</td>\n",
       "      <td>B005I5ZN0O</td>\n",
       "      <td>LoveAmazon</td>\n",
       "      <td>[6, 8]</td>\n",
       "      <td>I bought this chair for my son from Walmart fo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good concept...BAD QUALITY</td>\n",
       "      <td>1378252800</td>\n",
       "      <td>09 4, 2013</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Good concept...BAD QUALITY. I bought this chai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25490</th>\n",
       "      <td>AEEMMKYX27YQ</td>\n",
       "      <td>B009M7ZFYS</td>\n",
       "      <td>Amazonian 2082 \"Amazonian 2082\"</td>\n",
       "      <td>[4, 9]</td>\n",
       "      <td>This plunger looks great.  The magnetic base i...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Looks Great Haven't Had To Use It</td>\n",
       "      <td>1363651200</td>\n",
       "      <td>03 19, 2013</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>Looks Great Haven't Had To Use It. This plunge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12594</th>\n",
       "      <td>AY0WPNYO66YAA</td>\n",
       "      <td>B003GW873G</td>\n",
       "      <td>Don Weiser \"dweiser\"</td>\n",
       "      <td>[155, 172]</td>\n",
       "      <td>My Senseo finally died and I didn't feel like ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Use your Senseo Pods</td>\n",
       "      <td>1277769600</td>\n",
       "      <td>06 29, 2010</td>\n",
       "      <td>155</td>\n",
       "      <td>172</td>\n",
       "      <td>0.901163</td>\n",
       "      <td>Use your Senseo Pods. My Senseo finally died a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20848</th>\n",
       "      <td>A2B8GXSCB1R05T</td>\n",
       "      <td>B001GAQKKW</td>\n",
       "      <td>Zack Davisson</td>\n",
       "      <td>[117, 121]</td>\n",
       "      <td>I was looking to upgrade my measuring spoons r...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Measuring fun for everyone</td>\n",
       "      <td>1232928000</td>\n",
       "      <td>01 26, 2009</td>\n",
       "      <td>117</td>\n",
       "      <td>121</td>\n",
       "      <td>0.966942</td>\n",
       "      <td>Measuring fun for everyone. I was looking to u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39823</th>\n",
       "      <td>A1P0JXSAV0NFTW</td>\n",
       "      <td>B006462O94</td>\n",
       "      <td>N. Hubbard \"cross 2 cross\"</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>We have used many different types and styles o...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Now this is a bed</td>\n",
       "      <td>1366848000</td>\n",
       "      <td>04 25, 2013</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Now this is a bed. We have used many different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>AIBRTGBN07D6A</td>\n",
       "      <td>B005O37B28</td>\n",
       "      <td>Scott</td>\n",
       "      <td>[127, 132]</td>\n",
       "      <td>I've been cooking with a slow cooker for a cou...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Comparison with the Hamilton Beach Set 'n Forg...</td>\n",
       "      <td>1329177600</td>\n",
       "      <td>02 14, 2012</td>\n",
       "      <td>127</td>\n",
       "      <td>132</td>\n",
       "      <td>0.962121</td>\n",
       "      <td>Comparison with the Hamilton Beach Set 'n Forg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14451</th>\n",
       "      <td>A2JT19X5IZUVAH</td>\n",
       "      <td>B005QJ7OR2</td>\n",
       "      <td>PAN66</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>It broke the first time I used it.  The metal ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Broke</td>\n",
       "      <td>1366156800</td>\n",
       "      <td>04 17, 2013</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>Broke. It broke the first time I used it.  The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45849</th>\n",
       "      <td>A3LDYUGV9CW17W</td>\n",
       "      <td>B000OKHY2S</td>\n",
       "      <td>WSwendy</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>Well, I don't really expect to meet up with a ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Sure beats being eaten by a bear! Heheh... Gre...</td>\n",
       "      <td>1214784000</td>\n",
       "      <td>06 30, 2008</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>Sure beats being eaten by a bear! Heheh... Gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44000</th>\n",
       "      <td>A3A2QISZ44247F</td>\n",
       "      <td>B002MYZFKC</td>\n",
       "      <td>KB \"KB\"</td>\n",
       "      <td>[8, 11]</td>\n",
       "      <td>The humidifier part seems to work fine, but th...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Too many side affects</td>\n",
       "      <td>1362182400</td>\n",
       "      <td>03 2, 2013</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>Too many side affects. The humidifier part see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25869</th>\n",
       "      <td>A22HX8V6P30PHU</td>\n",
       "      <td>B0034EG5X0</td>\n",
       "      <td>Dorothy L. Sinkler</td>\n",
       "      <td>[21, 24]</td>\n",
       "      <td>I had a ring in my bath tub that nothing would...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>EXCELLENT PRODUCT</td>\n",
       "      <td>1269820800</td>\n",
       "      <td>03 29, 2010</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>EXCELLENT PRODUCT. I had a ring in my bath tub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>A1J2A38AU60GSD</td>\n",
       "      <td>B000IVAMBS</td>\n",
       "      <td>Crossfitter</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>It's comfortable, good size, has cup sizes mar...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love it!</td>\n",
       "      <td>1192579200</td>\n",
       "      <td>10 17, 2007</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>Love it!. It's comfortable, good size, has cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9608</th>\n",
       "      <td>A1Y1QDYBQ0G2V6</td>\n",
       "      <td>B005VT602W</td>\n",
       "      <td>M. Schmidt</td>\n",
       "      <td>[25, 30]</td>\n",
       "      <td>This is an initial review of the AirSpeed ABS ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AirSpeed ABS AS1050A review (has turbo brush i...</td>\n",
       "      <td>1324684800</td>\n",
       "      <td>12 24, 2011</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>AirSpeed ABS AS1050A review (has turbo brush i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33887</th>\n",
       "      <td>A3EFSLEMHNPP6A</td>\n",
       "      <td>B00005AL7F</td>\n",
       "      <td>Senor Zoidbergo</td>\n",
       "      <td>[7, 8]</td>\n",
       "      <td>What can I say, except that it's All-Clad?  We...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It's All-Clad!</td>\n",
       "      <td>1098489600</td>\n",
       "      <td>10 23, 2004</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>It's All-Clad!. What can I say, except that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28434</th>\n",
       "      <td>A3DBJQLY1MPM73</td>\n",
       "      <td>B001CA5LZ6</td>\n",
       "      <td>quiet bells \"quiet bells\"</td>\n",
       "      <td>[10, 14]</td>\n",
       "      <td>I have a earlier model of this sharpener. It w...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great sharpener, AVOID WAYFAIR</td>\n",
       "      <td>1322956800</td>\n",
       "      <td>12 4, 2011</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>Great sharpener, AVOID WAYFAIR. I have a earli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20971</th>\n",
       "      <td>ATT57LO1X4SHQ</td>\n",
       "      <td>B006UJW4NQ</td>\n",
       "      <td>frenchtoast</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>This is a really pretty design and I thought i...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nice paper but expensive</td>\n",
       "      <td>1342915200</td>\n",
       "      <td>07 22, 2012</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>nice paper but expensive. This is a really pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751</th>\n",
       "      <td>A3SYE6NEO8VSU0</td>\n",
       "      <td>B005BWMMCQ</td>\n",
       "      <td>Stars</td>\n",
       "      <td>[110, 112]</td>\n",
       "      <td>The Breville four-slice toaster will amaze.  M...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>\"A High Quality Toaster\"</td>\n",
       "      <td>1316563200</td>\n",
       "      <td>09 21, 2011</td>\n",
       "      <td>110</td>\n",
       "      <td>112</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>\"A High Quality Toaster\". The Breville four-sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13989</th>\n",
       "      <td>A5RMME9GIC2T2</td>\n",
       "      <td>B001HL0I9Y</td>\n",
       "      <td>P. M Palmer \"Phil\"</td>\n",
       "      <td>[16, 29]</td>\n",
       "      <td>People, you can make butter without this contr...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What?</td>\n",
       "      <td>1341619200</td>\n",
       "      <td>07 7, 2012</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>What?. People, you can make butter without thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8237</th>\n",
       "      <td>A4UFU4WKYEUGO</td>\n",
       "      <td>B002E1AWZ6</td>\n",
       "      <td>Rhonda Roberts</td>\n",
       "      <td>[32, 34]</td>\n",
       "      <td>I have a lot of Pinzon sheet sets. This is the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Pinzon quality - best overall for the price</td>\n",
       "      <td>1261440000</td>\n",
       "      <td>12 22, 2009</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>Pinzon quality - best overall for the price. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42620</th>\n",
       "      <td>A1C5DQKPBR4SGM</td>\n",
       "      <td>B0002ISJEY</td>\n",
       "      <td>J. Torres</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>Have to be careful when microwaving.  Lid pops...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Grab-It Dish with Cover</td>\n",
       "      <td>1183334400</td>\n",
       "      <td>07 2, 2007</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Grab-It Dish with Cover. Have to be careful wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11272</th>\n",
       "      <td>A31PK8GRU941BY</td>\n",
       "      <td>B000EZKYHA</td>\n",
       "      <td>New Englander</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>I'm glad that the other reviewers are loving t...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hard to see what your peeling...like maybe you...</td>\n",
       "      <td>1258502400</td>\n",
       "      <td>11 18, 2009</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Hard to see what your peeling...like maybe you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45470</th>\n",
       "      <td>A1EARN5PUVIF1S</td>\n",
       "      <td>B0000AR7SY</td>\n",
       "      <td>Jerry P. Danzig</td>\n",
       "      <td>[31, 34]</td>\n",
       "      <td>After I witnessed several puffs of white smoke...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fine Conical Burr Grinder</td>\n",
       "      <td>1152403200</td>\n",
       "      <td>07 9, 2006</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>Fine Conical Burr Grinder. After I witnessed s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31649</th>\n",
       "      <td>A2A3E45ES636ZN</td>\n",
       "      <td>B005FYF7XQ</td>\n",
       "      <td>Roman Cecilia</td>\n",
       "      <td>[28, 30]</td>\n",
       "      <td>I have to say this is the best and most econom...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great Deep Fryer</td>\n",
       "      <td>1385251200</td>\n",
       "      <td>11 24, 2013</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>Great Deep Fryer. I have to say this is the be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14060</th>\n",
       "      <td>A1FPP55KAZ8J2W</td>\n",
       "      <td>B00556H5MU</td>\n",
       "      <td>NostalgicMom</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>I used the this meat tenderizer yesterday for ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Surprised</td>\n",
       "      <td>1342742400</td>\n",
       "      <td>07 20, 2012</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Surprised. I used the this meat tenderizer yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38771</th>\n",
       "      <td>A1R4WPXHFLIH5X</td>\n",
       "      <td>B000KKIP3Y</td>\n",
       "      <td>Mbella</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>I read the reviews and was very excited to rec...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Not for medium to large cans</td>\n",
       "      <td>1306972800</td>\n",
       "      <td>06 2, 2011</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Not for medium to large cans. I read the revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>A680RUE1FDO8B</td>\n",
       "      <td>B001H1GPN2</td>\n",
       "      <td>Jerry Saperstein</td>\n",
       "      <td>[38, 39]</td>\n",
       "      <td>It's been pretty hot in Chicago these past two...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Very nice desktop/tabletop personal fan</td>\n",
       "      <td>1278633600</td>\n",
       "      <td>07 9, 2010</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>Very nice desktop/tabletop personal fan. It's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42376</th>\n",
       "      <td>AT6CZDCP4TRGA</td>\n",
       "      <td>B00005UP2E</td>\n",
       "      <td>Eduarrdo Nietzsche</td>\n",
       "      <td>[15, 18]</td>\n",
       "      <td>Nice packaging, convenient canister design is ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>you get what you pay for...</td>\n",
       "      <td>1066608000</td>\n",
       "      <td>10 20, 2003</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>you get what you pay for.... Nice packaging, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29897</th>\n",
       "      <td>A2582KMXLK2P06</td>\n",
       "      <td>B00007J5U7</td>\n",
       "      <td>B. E Jackson</td>\n",
       "      <td>[7, 12]</td>\n",
       "      <td>The Neuro Fuzzy Rice Cooker is relatively simp...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nice rice maker</td>\n",
       "      <td>1153180800</td>\n",
       "      <td>07 18, 2006</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>nice rice maker. The Neuro Fuzzy Rice Cooker i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23823</th>\n",
       "      <td>A2BLFCOPSMBOZ9</td>\n",
       "      <td>B000FKERMW</td>\n",
       "      <td>Dave Edmiston</td>\n",
       "      <td>[39, 43]</td>\n",
       "      <td>Filling hip flasks is a humongous pain.  You r...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Perfect for the drinker on-the-go</td>\n",
       "      <td>1280188800</td>\n",
       "      <td>07 27, 2010</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>Perfect for the drinker on-the-go. Filling hip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20109</th>\n",
       "      <td>A1EARN5PUVIF1S</td>\n",
       "      <td>B002F8IFV6</td>\n",
       "      <td>Jerry P. Danzig</td>\n",
       "      <td>[251, 288]</td>\n",
       "      <td>Yes, this little puppy really does work as adv...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Cool Cooler Coolest</td>\n",
       "      <td>1271289600</td>\n",
       "      <td>04 15, 2010</td>\n",
       "      <td>251</td>\n",
       "      <td>288</td>\n",
       "      <td>0.871528</td>\n",
       "      <td>Cool Cooler Coolest. Yes, this little puppy re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13319</th>\n",
       "      <td>AW6J9IILGOZY3</td>\n",
       "      <td>B003TMCNI8</td>\n",
       "      <td>Katawampas</td>\n",
       "      <td>[79, 83]</td>\n",
       "      <td>I love single malt whisky &amp; US bourbon.  I was...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Enhance your enjoyment of single malt whisky.</td>\n",
       "      <td>1294185600</td>\n",
       "      <td>01 5, 2011</td>\n",
       "      <td>79</td>\n",
       "      <td>83</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>Enhance your enjoyment of single malt whisky.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35639</th>\n",
       "      <td>AYPEPA3VS4KDM</td>\n",
       "      <td>B004ZZPFA6</td>\n",
       "      <td>Librarian</td>\n",
       "      <td>[16, 17]</td>\n",
       "      <td>I got this clip because I was so tired of tryi...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Works very well for most situations</td>\n",
       "      <td>1320019200</td>\n",
       "      <td>10 31, 2011</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>Works very well for most situations. I got thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28856</th>\n",
       "      <td>A2G4LKK57XMBL3</td>\n",
       "      <td>B00D6JUHUU</td>\n",
       "      <td>Sherry \"MomToTwoBoys\"</td>\n",
       "      <td>[5, 7]</td>\n",
       "      <td>I have been using the $300 technivorm drip cof...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Outstanding</td>\n",
       "      <td>1377993600</td>\n",
       "      <td>09 1, 2013</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>Outstanding. I have been using the $300 techni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45559</th>\n",
       "      <td>A2G18GEYP53EWC</td>\n",
       "      <td>B00E9BOAKM</td>\n",
       "      <td>Lisa M. D \"Lisa Marie\"</td>\n",
       "      <td>[8, 9]</td>\n",
       "      <td>I was SO excited when I saw this blanket, that...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I received the same quality from Wal Mart</td>\n",
       "      <td>1380412800</td>\n",
       "      <td>09 29, 2013</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>I received the same quality from Wal Mart. I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452</th>\n",
       "      <td>AXCNGPU7W5OGF</td>\n",
       "      <td>B004Z915M4</td>\n",
       "      <td>Maeven</td>\n",
       "      <td>[15, 17]</td>\n",
       "      <td>Most people serious about being raw will want ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Must Have for Raw Foodists</td>\n",
       "      <td>1293840000</td>\n",
       "      <td>01 1, 2011</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>Must Have for Raw Foodists. Most people seriou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30434 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID        asin  \\\n",
       "28393  A1JJOV69MAU2J2  B00004OCK0   \n",
       "27884  A1MAKVM3ATO1DC  B008BDSBEU   \n",
       "33598  A18H3X0XGMD1K0  B007D3V00Q   \n",
       "43366  A14CPE16HGLDW2  B005SI8YZC   \n",
       "2943   A2CGJON31CMTCR  B0049SHD7C   \n",
       "12726  A15FA4XWUDLMI7  B0000CFOIH   \n",
       "26378  A1TS45JWJVOSSW  B0016LQ01K   \n",
       "4133   A1Z54EM24Y40LL  B00004SGFS   \n",
       "23521  A3Q20W5LJTGC7T  B00164W8HM   \n",
       "9833   A3HNW0MW20ZIXF  B0001KOA4Q   \n",
       "30174  A2JP0URFHXP6DO  B0000E19MW   \n",
       "6910   A1BZN88T4GSZG0  B00DBTOVXO   \n",
       "2589   A2R5R53I1LMUDG  B001HI3YOS   \n",
       "18167  A2NJ6M5VQYEVHC  B0000BYC4B   \n",
       "26499  A2TDQ6U4FXYTNP  B000FZZ0VE   \n",
       "42864   A5BCJVGF1LA4O  B000G0OJ9C   \n",
       "11422  A3V6Z4RCDGRC44  B000COC5MK   \n",
       "30848  A3CBKR4HL03YR3  B0033XX5H6   \n",
       "27525  A14H5GNH2EIPAC  B007M0HNYC   \n",
       "33239  A23A3FEXAMBOIJ  B00005Q5K4   \n",
       "35259  A1PL0PBCQ6YC9E  B0006IQ2NA   \n",
       "5189   A2EFG35K3TA4VZ  B0000Z6JK0   \n",
       "35741  A3B62CEEBAKM7M  B001EX44C2   \n",
       "33778   AQZ1WKIJT3ZTT  B0007MS5B2   \n",
       "45978  A2IO1ESNSIAXG3  B00004S1B8   \n",
       "8835   A1G4BESDDK9679  B0084DAI3O   \n",
       "31759   ALNAXN9SFT7BB  B005I5ZN0O   \n",
       "25490    AEEMMKYX27YQ  B009M7ZFYS   \n",
       "12594   AY0WPNYO66YAA  B003GW873G   \n",
       "20848  A2B8GXSCB1R05T  B001GAQKKW   \n",
       "...               ...         ...   \n",
       "39823  A1P0JXSAV0NFTW  B006462O94   \n",
       "2182    AIBRTGBN07D6A  B005O37B28   \n",
       "14451  A2JT19X5IZUVAH  B005QJ7OR2   \n",
       "45849  A3LDYUGV9CW17W  B000OKHY2S   \n",
       "44000  A3A2QISZ44247F  B002MYZFKC   \n",
       "25869  A22HX8V6P30PHU  B0034EG5X0   \n",
       "1837   A1J2A38AU60GSD  B000IVAMBS   \n",
       "9608   A1Y1QDYBQ0G2V6  B005VT602W   \n",
       "33887  A3EFSLEMHNPP6A  B00005AL7F   \n",
       "28434  A3DBJQLY1MPM73  B001CA5LZ6   \n",
       "20971   ATT57LO1X4SHQ  B006UJW4NQ   \n",
       "9751   A3SYE6NEO8VSU0  B005BWMMCQ   \n",
       "13989   A5RMME9GIC2T2  B001HL0I9Y   \n",
       "8237    A4UFU4WKYEUGO  B002E1AWZ6   \n",
       "42620  A1C5DQKPBR4SGM  B0002ISJEY   \n",
       "11272  A31PK8GRU941BY  B000EZKYHA   \n",
       "45470  A1EARN5PUVIF1S  B0000AR7SY   \n",
       "31649  A2A3E45ES636ZN  B005FYF7XQ   \n",
       "14060  A1FPP55KAZ8J2W  B00556H5MU   \n",
       "38771  A1R4WPXHFLIH5X  B000KKIP3Y   \n",
       "8072    A680RUE1FDO8B  B001H1GPN2   \n",
       "42376   AT6CZDCP4TRGA  B00005UP2E   \n",
       "29897  A2582KMXLK2P06  B00007J5U7   \n",
       "23823  A2BLFCOPSMBOZ9  B000FKERMW   \n",
       "20109  A1EARN5PUVIF1S  B002F8IFV6   \n",
       "13319   AW6J9IILGOZY3  B003TMCNI8   \n",
       "35639   AYPEPA3VS4KDM  B004ZZPFA6   \n",
       "28856  A2G4LKK57XMBL3  B00D6JUHUU   \n",
       "45559  A2G18GEYP53EWC  B00E9BOAKM   \n",
       "8452    AXCNGPU7W5OGF  B004Z915M4   \n",
       "\n",
       "                                           reviewerName     helpful  \\\n",
       "28393                                     Steven Dennis      [6, 7]   \n",
       "27884                                            Kaylie    [25, 26]   \n",
       "33598                                               NaN      [1, 5]   \n",
       "43366                                  Sam Mercer \"Sam\"      [6, 8]   \n",
       "2943                                       silverorchid      [6, 9]   \n",
       "12726                       W. Peterson \"creative cook\"      [4, 5]   \n",
       "26378                              Duane Sparks \"Duane\"      [7, 8]   \n",
       "4133                                                csm      [8, 9]   \n",
       "23521                                           swrobel    [14, 18]   \n",
       "9833                           Dominika Lepak \"nikatoo\"  [104, 116]   \n",
       "30174                                        Tim Janson  [297, 311]   \n",
       "6910                                       Silly Sister    [10, 11]   \n",
       "2589                                    amazon addicted      [4, 5]   \n",
       "18167                                             T. F.  [103, 114]   \n",
       "26499                                     Maeve \"Maeve\"     [8, 27]   \n",
       "42864                                     Likes to Shop     [1, 20]   \n",
       "11422  Lisa Shea \"be the change you wish to see in t...  [170, 186]   \n",
       "30848                                        Sassy Lady    [21, 22]   \n",
       "27525                           Kuromame's Mommy \"Rose\"      [1, 5]   \n",
       "33239                                          fairview      [6, 8]   \n",
       "35259                                   Victor S. \"vs.\"    [19, 20]   \n",
       "5189                                           T. Dietz      [7, 8]   \n",
       "35741                                               USA    [19, 21]   \n",
       "33778                                      TheBigOldDog      [8, 9]   \n",
       "45978                                        L. A. Kane    [46, 47]   \n",
       "8835                              LesfromSanDiego \"Les\"    [21, 25]   \n",
       "31759                                        LoveAmazon      [6, 8]   \n",
       "25490                   Amazonian 2082 \"Amazonian 2082\"      [4, 9]   \n",
       "12594                              Don Weiser \"dweiser\"  [155, 172]   \n",
       "20848                                     Zack Davisson  [117, 121]   \n",
       "...                                                 ...         ...   \n",
       "39823                        N. Hubbard \"cross 2 cross\"     [9, 10]   \n",
       "2182                                              Scott  [127, 132]   \n",
       "14451                                             PAN66      [6, 7]   \n",
       "45849                                           WSwendy      [7, 8]   \n",
       "44000                                           KB \"KB\"     [8, 11]   \n",
       "25869                                Dorothy L. Sinkler    [21, 24]   \n",
       "1837                                        Crossfitter     [9, 10]   \n",
       "9608                                         M. Schmidt    [25, 30]   \n",
       "33887                                   Senor Zoidbergo      [7, 8]   \n",
       "28434                         quiet bells \"quiet bells\"    [10, 14]   \n",
       "20971                                       frenchtoast     [9, 10]   \n",
       "9751                                              Stars  [110, 112]   \n",
       "13989                                P. M Palmer \"Phil\"    [16, 29]   \n",
       "8237                                     Rhonda Roberts    [32, 34]   \n",
       "42620                                         J. Torres      [4, 5]   \n",
       "11272                                     New Englander      [4, 5]   \n",
       "45470                                   Jerry P. Danzig    [31, 34]   \n",
       "31649                                     Roman Cecilia    [28, 30]   \n",
       "14060                                      NostalgicMom      [8, 9]   \n",
       "38771                                            Mbella      [4, 5]   \n",
       "8072                                   Jerry Saperstein    [38, 39]   \n",
       "42376                                Eduarrdo Nietzsche    [15, 18]   \n",
       "29897                                      B. E Jackson     [7, 12]   \n",
       "23823                                     Dave Edmiston    [39, 43]   \n",
       "20109                                   Jerry P. Danzig  [251, 288]   \n",
       "13319                                        Katawampas    [79, 83]   \n",
       "35639                                         Librarian    [16, 17]   \n",
       "28856                             Sherry \"MomToTwoBoys\"      [5, 7]   \n",
       "45559                            Lisa M. D \"Lisa Marie\"      [8, 9]   \n",
       "8452                                             Maeven    [15, 17]   \n",
       "\n",
       "                                              reviewText  overall  \\\n",
       "28393  These tongs are cleverly designed and solidly ...      5.0   \n",
       "27884  I was skeptical to buy this because I was worr...      5.0   \n",
       "33598  I have not used this press because it will be ...      3.0   \n",
       "43366  Steam blown.  Wow, this thing really gets hot....      5.0   \n",
       "2943   I have had several waffle makers. Since most o...      3.0   \n",
       "12726  I really like using this for those special lit...      5.0   \n",
       "26378  Cooks and cleans perfectly, and has already be...      5.0   \n",
       "4133   If you're bored some day, buy these attachment...      5.0   \n",
       "23521  Well, I think for once the old adage \"you get ...      4.0   \n",
       "9833   For the price, I'd give the espresso making pa...      4.0   \n",
       "30174  I've prided myself on my homemade pizza for ye...      5.0   \n",
       "6910   Overall I'm pleased with the Bissell Carpet Cl...      4.0   \n",
       "2589   These jars are really nice for the price. They...      5.0   \n",
       "18167  According to the manufacturer's web site, Ball...      5.0   \n",
       "26499  This breadmaker makes good bread, but plan to ...      3.0   \n",
       "42864  This is a great little heater thats really pow...      5.0   \n",
       "11422  I really had high hopes for the Oster. We open...      2.0   \n",
       "30848  I was thrilled to have found this set at such ...      5.0   \n",
       "27525  I got this as a freebie with another order.  I...      4.0   \n",
       "33239  This teakettle was given to me as a gift. I wi...      2.0   \n",
       "35259  This is an OK item. I did not want to have 3 s...      3.0   \n",
       "5189   I bought this thing thinking believing others ...      2.0   \n",
       "35741  I had high hopes for this after reading all th...      1.0   \n",
       "33778  I purchased 2 of these when  they first came o...      1.0   \n",
       "45978  Nothing beats a good whetstone, but you need t...      4.0   \n",
       "8835   We really like the air circulation with this V...      5.0   \n",
       "31759  I bought this chair for my son from Walmart fo...      1.0   \n",
       "25490  This plunger looks great.  The magnetic base i...      4.0   \n",
       "12594  My Senseo finally died and I didn't feel like ...      4.0   \n",
       "20848  I was looking to upgrade my measuring spoons r...      4.0   \n",
       "...                                                  ...      ...   \n",
       "39823  We have used many different types and styles o...      5.0   \n",
       "2182   I've been cooking with a slow cooker for a cou...      4.0   \n",
       "14451  It broke the first time I used it.  The metal ...      1.0   \n",
       "45849  Well, I don't really expect to meet up with a ...      5.0   \n",
       "44000  The humidifier part seems to work fine, but th...      3.0   \n",
       "25869  I had a ring in my bath tub that nothing would...      5.0   \n",
       "1837   It's comfortable, good size, has cup sizes mar...      5.0   \n",
       "9608   This is an initial review of the AirSpeed ABS ...      5.0   \n",
       "33887  What can I say, except that it's All-Clad?  We...      5.0   \n",
       "28434  I have a earlier model of this sharpener. It w...      5.0   \n",
       "20971  This is a really pretty design and I thought i...      4.0   \n",
       "9751   The Breville four-slice toaster will amaze.  M...      5.0   \n",
       "13989  People, you can make butter without this contr...      1.0   \n",
       "8237   I have a lot of Pinzon sheet sets. This is the...      5.0   \n",
       "42620  Have to be careful when microwaving.  Lid pops...      2.0   \n",
       "11272  I'm glad that the other reviewers are loving t...      2.0   \n",
       "45470  After I witnessed several puffs of white smoke...      5.0   \n",
       "31649  I have to say this is the best and most econom...      5.0   \n",
       "14060  I used the this meat tenderizer yesterday for ...      4.0   \n",
       "38771  I read the reviews and was very excited to rec...      2.0   \n",
       "8072   It's been pretty hot in Chicago these past two...      5.0   \n",
       "42376  Nice packaging, convenient canister design is ...      2.0   \n",
       "29897  The Neuro Fuzzy Rice Cooker is relatively simp...      5.0   \n",
       "23823  Filling hip flasks is a humongous pain.  You r...      5.0   \n",
       "20109  Yes, this little puppy really does work as adv...      5.0   \n",
       "13319  I love single malt whisky & US bourbon.  I was...      5.0   \n",
       "35639  I got this clip because I was so tired of tryi...      4.0   \n",
       "28856  I have been using the $300 technivorm drip cof...      5.0   \n",
       "45559  I was SO excited when I saw this blanket, that...      3.0   \n",
       "8452   Most people serious about being raw will want ...      5.0   \n",
       "\n",
       "                                                 summary  unixReviewTime  \\\n",
       "28393       The best pair of tongs at a bargain price...       999820800   \n",
       "27884                                      Beautiful!!!!      1358985600   \n",
       "33598                         Will be difficult to clean      1369785600   \n",
       "43366                                   My mind is blown      1355788800   \n",
       "2943                      Average product with few flaws      1364428800   \n",
       "12726                   Nice little tool for the kitchen      1233100800   \n",
       "26378                                    Perfect product      1314230400   \n",
       "4133                                 Awesome attachment!      1104451200   \n",
       "23521                 You get more than what you pay for      1243900800   \n",
       "9833                  Great espresso, poor steaming wand      1111968000   \n",
       "30174                           THE BEST HOME PIZZA YET!      1108598400   \n",
       "6910                                    Worth the price!      1384387200   \n",
       "2589                            Nice Jars for the price!      1349136000   \n",
       "18167                 Does not contain bisphenol A (BPA)      1314576000   \n",
       "26499                                 Good bread, but...      1170806400   \n",
       "42864                                      Great Heater!      1163721600   \n",
       "11422              Nice Idea - Poor Long Term Usefulness      1222992000   \n",
       "30848                       Great Price on Great Product      1278028800   \n",
       "27525                              digital kitchen scale      1347840000   \n",
       "33239                      Disappointed with this teapot      1116892800   \n",
       "35259                      10 inch - what it stands for?      1305504000   \n",
       "5189   overheats.  worthless for anything but chicken...      1331596800   \n",
       "35741                          I had high hopes for this      1275782400   \n",
       "33778           Save your money and buy a Scooba instead      1201219200   \n",
       "45978              Nothing beats a good whetstone but...      1145145600   \n",
       "8835        Great Fan. Does exactly what it claims to do      1369612800   \n",
       "31759                         Good concept...BAD QUALITY      1378252800   \n",
       "25490                  Looks Great Haven't Had To Use It      1363651200   \n",
       "12594                               Use your Senseo Pods      1277769600   \n",
       "20848                         Measuring fun for everyone      1232928000   \n",
       "...                                                  ...             ...   \n",
       "39823                                  Now this is a bed      1366848000   \n",
       "2182   Comparison with the Hamilton Beach Set 'n Forg...      1329177600   \n",
       "14451                                              Broke      1366156800   \n",
       "45849  Sure beats being eaten by a bear! Heheh... Gre...      1214784000   \n",
       "44000                              Too many side affects      1362182400   \n",
       "25869                                  EXCELLENT PRODUCT      1269820800   \n",
       "1837                                            Love it!      1192579200   \n",
       "9608   AirSpeed ABS AS1050A review (has turbo brush i...      1324684800   \n",
       "33887                                     It's All-Clad!      1098489600   \n",
       "28434                     Great sharpener, AVOID WAYFAIR      1322956800   \n",
       "20971                           nice paper but expensive      1342915200   \n",
       "9751                            \"A High Quality Toaster\"      1316563200   \n",
       "13989                                              What?      1341619200   \n",
       "8237         Pinzon quality - best overall for the price      1261440000   \n",
       "42620                            Grab-It Dish with Cover      1183334400   \n",
       "11272  Hard to see what your peeling...like maybe you...      1258502400   \n",
       "45470                          Fine Conical Burr Grinder      1152403200   \n",
       "31649                                   Great Deep Fryer      1385251200   \n",
       "14060                                          Surprised      1342742400   \n",
       "38771                       Not for medium to large cans      1306972800   \n",
       "8072             Very nice desktop/tabletop personal fan      1278633600   \n",
       "42376                        you get what you pay for...      1066608000   \n",
       "29897                                    nice rice maker      1153180800   \n",
       "23823                  Perfect for the drinker on-the-go      1280188800   \n",
       "20109                                Cool Cooler Coolest      1271289600   \n",
       "13319      Enhance your enjoyment of single malt whisky.      1294185600   \n",
       "35639                Works very well for most situations      1320019200   \n",
       "28856                                        Outstanding      1377993600   \n",
       "45559          I received the same quality from Wal Mart      1380412800   \n",
       "8452                          Must Have for Raw Foodists      1293840000   \n",
       "\n",
       "        reviewTime  helpful_votes  total_votes  helpful_perc  \\\n",
       "28393   09 7, 2001              6            7      0.857143   \n",
       "27884  01 24, 2013             25           26      0.961538   \n",
       "33598  05 29, 2013              1            5      0.200000   \n",
       "43366  12 18, 2012              6            8      0.750000   \n",
       "2943   03 28, 2013              6            9      0.666667   \n",
       "12726  01 28, 2009              4            5      0.800000   \n",
       "26378  08 25, 2011              7            8      0.875000   \n",
       "4133   12 31, 2004              8            9      0.888889   \n",
       "23521   06 2, 2009             14           18      0.777778   \n",
       "9833   03 28, 2005            104          116      0.896552   \n",
       "30174  02 17, 2005            297          311      0.954984   \n",
       "6910   11 14, 2013             10           11      0.909091   \n",
       "2589    10 2, 2012              4            5      0.800000   \n",
       "18167  08 29, 2011            103          114      0.903509   \n",
       "26499   02 7, 2007              8           27      0.296296   \n",
       "42864  11 17, 2006              1           20      0.050000   \n",
       "11422   10 3, 2008            170          186      0.913978   \n",
       "30848   07 2, 2010             21           22      0.954545   \n",
       "27525  09 17, 2012              1            5      0.200000   \n",
       "33239  05 24, 2005              6            8      0.750000   \n",
       "35259  05 16, 2011             19           20      0.950000   \n",
       "5189   03 13, 2012              7            8      0.875000   \n",
       "35741   06 6, 2010             19           21      0.904762   \n",
       "33778  01 25, 2008              8            9      0.888889   \n",
       "45978  04 16, 2006             46           47      0.978723   \n",
       "8835   05 27, 2013             21           25      0.840000   \n",
       "31759   09 4, 2013              6            8      0.750000   \n",
       "25490  03 19, 2013              4            9      0.444444   \n",
       "12594  06 29, 2010            155          172      0.901163   \n",
       "20848  01 26, 2009            117          121      0.966942   \n",
       "...            ...            ...          ...           ...   \n",
       "39823  04 25, 2013              9           10      0.900000   \n",
       "2182   02 14, 2012            127          132      0.962121   \n",
       "14451  04 17, 2013              6            7      0.857143   \n",
       "45849  06 30, 2008              7            8      0.875000   \n",
       "44000   03 2, 2013              8           11      0.727273   \n",
       "25869  03 29, 2010             21           24      0.875000   \n",
       "1837   10 17, 2007              9           10      0.900000   \n",
       "9608   12 24, 2011             25           30      0.833333   \n",
       "33887  10 23, 2004              7            8      0.875000   \n",
       "28434   12 4, 2011             10           14      0.714286   \n",
       "20971  07 22, 2012              9           10      0.900000   \n",
       "9751   09 21, 2011            110          112      0.982143   \n",
       "13989   07 7, 2012             16           29      0.551724   \n",
       "8237   12 22, 2009             32           34      0.941176   \n",
       "42620   07 2, 2007              4            5      0.800000   \n",
       "11272  11 18, 2009              4            5      0.800000   \n",
       "45470   07 9, 2006             31           34      0.911765   \n",
       "31649  11 24, 2013             28           30      0.933333   \n",
       "14060  07 20, 2012              8            9      0.888889   \n",
       "38771   06 2, 2011              4            5      0.800000   \n",
       "8072    07 9, 2010             38           39      0.974359   \n",
       "42376  10 20, 2003             15           18      0.833333   \n",
       "29897  07 18, 2006              7           12      0.583333   \n",
       "23823  07 27, 2010             39           43      0.906977   \n",
       "20109  04 15, 2010            251          288      0.871528   \n",
       "13319   01 5, 2011             79           83      0.951807   \n",
       "35639  10 31, 2011             16           17      0.941176   \n",
       "28856   09 1, 2013              5            7      0.714286   \n",
       "45559  09 29, 2013              8            9      0.888889   \n",
       "8452    01 1, 2011             15           17      0.882353   \n",
       "\n",
       "                                            combinedText  \n",
       "28393  The best pair of tongs at a bargain price.... ...  \n",
       "27884  Beautiful!!!!. I was skeptical to buy this bec...  \n",
       "33598  Will be difficult to clean. I have not used th...  \n",
       "43366  My mind is blown. Steam blown.  Wow, this thin...  \n",
       "2943   Average product with few flaws. I have had sev...  \n",
       "12726  Nice little tool for the kitchen. I really lik...  \n",
       "26378  Perfect product. Cooks and cleans perfectly, a...  \n",
       "4133   Awesome attachment!. If you're bored some day,...  \n",
       "23521  You get more than what you pay for. Well, I th...  \n",
       "9833   Great espresso, poor steaming wand. For the pr...  \n",
       "30174  THE BEST HOME PIZZA YET!. I've prided myself o...  \n",
       "6910   Worth the price!. Overall I'm pleased with the...  \n",
       "2589   Nice Jars for the price!. These jars are reall...  \n",
       "18167  Does not contain bisphenol A (BPA). According ...  \n",
       "26499  Good bread, but.... This breadmaker makes good...  \n",
       "42864  Great Heater!. This is a great little heater t...  \n",
       "11422  Nice Idea - Poor Long Term Usefulness. I reall...  \n",
       "30848  Great Price on Great Product. I was thrilled t...  \n",
       "27525  digital kitchen scale. I got this as a freebie...  \n",
       "33239  Disappointed with this teapot. This teakettle ...  \n",
       "35259  10 inch - what it stands for?. This is an OK i...  \n",
       "5189   overheats.  worthless for anything but chicken...  \n",
       "35741  I had high hopes for this. I had high hopes fo...  \n",
       "33778  Save your money and buy a Scooba instead. I pu...  \n",
       "45978  Nothing beats a good whetstone but.... Nothing...  \n",
       "8835   Great Fan. Does exactly what it claims to do. ...  \n",
       "31759  Good concept...BAD QUALITY. I bought this chai...  \n",
       "25490  Looks Great Haven't Had To Use It. This plunge...  \n",
       "12594  Use your Senseo Pods. My Senseo finally died a...  \n",
       "20848  Measuring fun for everyone. I was looking to u...  \n",
       "...                                                  ...  \n",
       "39823  Now this is a bed. We have used many different...  \n",
       "2182   Comparison with the Hamilton Beach Set 'n Forg...  \n",
       "14451  Broke. It broke the first time I used it.  The...  \n",
       "45849  Sure beats being eaten by a bear! Heheh... Gre...  \n",
       "44000  Too many side affects. The humidifier part see...  \n",
       "25869  EXCELLENT PRODUCT. I had a ring in my bath tub...  \n",
       "1837   Love it!. It's comfortable, good size, has cup...  \n",
       "9608   AirSpeed ABS AS1050A review (has turbo brush i...  \n",
       "33887  It's All-Clad!. What can I say, except that it...  \n",
       "28434  Great sharpener, AVOID WAYFAIR. I have a earli...  \n",
       "20971  nice paper but expensive. This is a really pre...  \n",
       "9751   \"A High Quality Toaster\". The Breville four-sl...  \n",
       "13989  What?. People, you can make butter without thi...  \n",
       "8237   Pinzon quality - best overall for the price. I...  \n",
       "42620  Grab-It Dish with Cover. Have to be careful wh...  \n",
       "11272  Hard to see what your peeling...like maybe you...  \n",
       "45470  Fine Conical Burr Grinder. After I witnessed s...  \n",
       "31649  Great Deep Fryer. I have to say this is the be...  \n",
       "14060  Surprised. I used the this meat tenderizer yes...  \n",
       "38771  Not for medium to large cans. I read the revie...  \n",
       "8072   Very nice desktop/tabletop personal fan. It's ...  \n",
       "42376  you get what you pay for.... Nice packaging, c...  \n",
       "29897  nice rice maker. The Neuro Fuzzy Rice Cooker i...  \n",
       "23823  Perfect for the drinker on-the-go. Filling hip...  \n",
       "20109  Cool Cooler Coolest. Yes, this little puppy re...  \n",
       "13319  Enhance your enjoyment of single malt whisky.....  \n",
       "35639  Works very well for most situations. I got thi...  \n",
       "28856  Outstanding. I have been using the $300 techni...  \n",
       "45559  I received the same quality from Wal Mart. I w...  \n",
       "8452   Must Have for Raw Foodists. Most people seriou...  \n",
       "\n",
       "[30434 rows x 13 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = df_train\n",
    "\n",
    "bert_df['isHelpful'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = MAX_SEQ_LENGTH\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'n_fine_tune_layers': self.n_fine_tune_layers,\n",
    "                  'pooling': self.pooling,\n",
    "                  'bert_path': self.bert_path}\n",
    "        base_config = super(MyMeanPooling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Build model\n",
    "def build_model(max_seq_length, model_loss, reg=False): \n",
    "\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3, pooling=\"first\")(bert_inputs)\n",
    "    if reg:\n",
    "        dense = tf.keras.layers.Dense(max_seq_length, activation='relu', kernel_regularizer=regularizers.l2(0.01))(bert_output)\n",
    "    else:\n",
    "        dense = tf.keras.layers.Dense(max_seq_length, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss=model_loss, optimizer='adam')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model without explicit bert layer\n",
    "def build_model_bert(max_seq_length, model_loss, reg=False): \n",
    "\n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    x = tf.keras.layers.concatenate(bert_inputs)\n",
    "    \n",
    "    if reg:\n",
    "        dense = tf.keras.layers.Dense(max_seq_length, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    else:\n",
    "        dense = tf.keras.layers.Dense(max_seq_length, activation='relu')(x)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss=model_loss, optimizer='adam')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_15 (BertLayer)       (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 128)          98432       bert_layer_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1)            129         dense_24[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 110,203,451\n",
      "Trainable params: 21,952,769\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_16 (BertLayer)       (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 128)          98432       bert_layer_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            129         dense_26[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 110,203,451\n",
      "Trainable params: 21,952,769\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# let's define some models!\n",
    "\n",
    "#BCE Model: using BCE in regression https://stats.stackexchange.com/questions/370179/why-binary-crossentropy-can-be-used-as-the-loss-function-in-autoencoders/370180#370180\n",
    "model_BCE = build_model(MAX_SEQ_LENGTH, 'binary_crossentropy')\n",
    "model_RMSE = build_model(MAX_SEQ_LENGTH, 'mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [768,3072] and type float\n\t [[node training/Adam/bert_layer_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training/Adam/bert_layer_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training/Adam/bert_layer_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_2/Adam/bert_layer_1_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_2/Adam/bert_layer_1_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_2/Adam/bert_layer_1_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_4/Adam/bert_layer_5_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_4/Adam/bert_layer_5_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_4/Adam/bert_layer_5_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_6/Adam/bert_layer_6_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_6/Adam/bert_layer_6_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_6/Adam/bert_layer_6_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n\nOriginal stack trace for 'training/Adam/bert_layer_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros':\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-fe3b1540172f>\", line 10, in <module>\n    batch_size=8\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 727, in fit\n    use_multiprocessing=use_multiprocessing)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 675, in fit\n    steps_name='steps_per_epoch')\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 189, in model_iteration\n    f = _make_execution_function(model, mode)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 566, in _make_execution_function\n    return model._make_execution_function(mode)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2183, in _make_execution_function\n    self._make_train_function()\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2115, in _make_train_function\n    params=self._collected_trainable_weights, loss=self.total_loss)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 504, in get_updates\n    return [self.apply_gradients(grads_and_vars)]\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 433, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adam.py\", line 149, in _create_slots\n    self.add_slot(var, 'm')\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 585, in add_slot\n    initial_value=initial_value)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 260, in __call__\n    return cls._variable_v2_call(*args, **kwargs)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 254, in _variable_v2_call\n    shape=shape)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 235, in <lambda>\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2552, in default_variable_creator_v2\n    shape=shape)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1406, in __init__\n    distribute_strategy=distribute_strategy)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1537, in _init_from_args\n    initial_value() if init_from_fn else initial_value,\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py\", line 114, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2350, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 171, in fill\n    result = gen_array_ops.fill(dims, value, name=name)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 3602, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [768,3072] and type float\n\t [[{{node training/Adam/bert_layer_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training/Adam/bert_layer_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training/Adam/bert_layer_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_2/Adam/bert_layer_1_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_2/Adam/bert_layer_1_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_2/Adam/bert_layer_1_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_4/Adam/bert_layer_5_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_4/Adam/bert_layer_5_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_4/Adam/bert_layer_5_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_6/Adam/bert_layer_6_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_6/Adam/bert_layer_6_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros}}, {{node training_6/Adam/bert_layer_6_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-215-fac4e937dfa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#we can now finally build a model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Instantiate variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minitialize_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model_BCE.fit(\n",
      "\u001b[1;32m<ipython-input-213-3e68b8e77935>\u001b[0m in \u001b[0;36minitialize_vars\u001b[1;34m(sess)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minitialize_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [768,3072] and type float\n\t [[node training/Adam/bert_layer_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training/Adam/bert_layer_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training/Adam/bert_layer_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_2/Adam/bert_layer_1_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_2/Adam/bert_layer_1_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_2/Adam/bert_layer_1_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_4/Adam/bert_layer_5_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_4/Adam/bert_layer_5_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_4/Adam/bert_layer_5_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_6/Adam/bert_layer_6_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_6/Adam/bert_layer_6_module/bert/encoder/layer_11/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) , node training_6/Adam/bert_layer_6_module/bert/encoder/layer_9/intermediate/dense/kernel/m/Initializer/zeros (defined at C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\n\nOriginal stack trace for 'training/Adam/bert_layer_module/bert/encoder/layer_10/intermediate/dense/kernel/m/Initializer/zeros':\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-fe3b1540172f>\", line 10, in <module>\n    batch_size=8\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 727, in fit\n    use_multiprocessing=use_multiprocessing)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 675, in fit\n    steps_name='steps_per_epoch')\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 189, in model_iteration\n    f = _make_execution_function(model, mode)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 566, in _make_execution_function\n    return model._make_execution_function(mode)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2183, in _make_execution_function\n    self._make_train_function()\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 2115, in _make_train_function\n    params=self._collected_trainable_weights, loss=self.total_loss)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 504, in get_updates\n    return [self.apply_gradients(grads_and_vars)]\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 433, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\adam.py\", line 149, in _create_slots\n    self.add_slot(var, 'm')\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 585, in add_slot\n    initial_value=initial_value)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 260, in __call__\n    return cls._variable_v2_call(*args, **kwargs)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 254, in _variable_v2_call\n    shape=shape)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 235, in <lambda>\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variable_scope.py\", line 2552, in default_variable_creator_v2\n    shape=shape)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1406, in __init__\n    distribute_strategy=distribute_strategy)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 1537, in _init_from_args\n    initial_value() if init_from_fn else initial_value,\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py\", line 114, in __call__\n    return array_ops.zeros(shape, dtype)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 2350, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 171, in fill\n    result = gen_array_ops.fill(dims, value, name=name)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 3602, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "#we can now finally build a model\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model_BCE.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=1,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "model_BCE.save_weights('./models/model_BCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's test the correlation coefficient\n",
    "preds_BCE = model_BCE.predict([test_input_ids, test_input_masks, test_segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.corrcoef(preds_BCE.T, test_actual.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's try RMSE\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model_RMSE.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=1,\n",
    "    batch_size=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "model_RMSE.save_weights('./models/model_RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's test the correlation coefficient\n",
    "preds_RMSE = model_RMSE.predict([test_input_ids, test_input_masks, test_segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan],\n",
       "       [nan,  1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds_RMSE.T, test_actual.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_6 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          196864      bert_layer_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            257         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 22,051,329\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n",
      "Train on 10000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 1404s 140ms/sample - loss: 0.2731 - val_loss: 0.1117\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 387s 39ms/sample - loss: 0.0999 - val_loss: 0.0937\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 387s 39ms/sample - loss: 0.0978 - val_loss: 0.0990\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 387s 39ms/sample - loss: 0.0970 - val_loss: 0.0932\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 387s 39ms/sample - loss: 0.0974 - val_loss: 0.0931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2076f961860>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's retry with some regularization\n",
    "model_RMSE_reg = build_model(MAX_SEQ_LENGTH, 'mean_squared_error', reg=True)\n",
    "\n",
    "# let's try RMSE\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model_RMSE_reg.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=5,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save weights\n",
    "model_RMSE_reg.save_weights('./models/model_RMSE_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's test the correlation coefficient\n",
    "preds_RMSE_reg = model_RMSE_reg.predict([test_input_ids, test_input_masks, test_segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8183431],\n",
       "       [0.8183431],\n",
       "       [0.8183431],\n",
       "       ...,\n",
       "       [0.8183431],\n",
       "       [0.8183431],\n",
       "       [0.8183431]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_RMSE_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "C:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan, nan],\n",
       "       [nan,  1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(preds_RMSE_reg.T, test_actual.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Model\n",
    "We'll now try and implement review from other domains, not just the home and kitchen reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load extra datasets - download directly from source, save to data directory\n",
    "\n",
    "output_dir = \"data/\"\n",
    "url = \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/\"\n",
    "\n",
    "kitchen_file_name = \"reviews_Home_and_Kitchen_5.json.gz\"\n",
    "cell_file_name = \"reviews_Cell_Phones_and_Accessories_5.json.gz\"\n",
    "clothing_file_name = \"reviews_Clothing_Shoes_and_Jewelry_5.json.gz\"\n",
    "outdoor_file_name = \"reviews_Sports_and_Outdoors_5.json.gz\"\n",
    "electronics_file_name = \"reviews_Electronics_5.json.gz\"\n",
    "\n",
    "file_names = [kitchen_file_name, cell_file_name, clothing_file_name, outdoor_file_name, electronics_file_name]\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for file in file_names:\n",
    "    if not os.path.isfile(output_dir + file):\n",
    "        file_name = wget.download(url + file, out=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "df_kitchen = get_dataframe(output_dir + kitchen_file_name)\n",
    "df_cell = get_dataframe(output_dir + cell_file_name)\n",
    "df_clothing = get_dataframe(output_dir + clothing_file_name)\n",
    "df_outdoor = get_dataframe(output_dir + outdoor_file_name)\n",
    "df_electronics = get_dataframe(output_dir + electronics_file_name)\n",
    "\n",
    "dfs = [df_kitchen, df_cell, df_clothing, df_outdoor, df_electronics]\n",
    "\n",
    "# parse helpful column into new columns of helpful_votes, total_votes, helpful_perc\n",
    "for i, df in enumerate(dfs):\n",
    "    df['helpful_votes'] = df['helpful'].apply(get_helpful_votes)\n",
    "    df['total_votes'] = df['helpful'].apply(get_total_votes)\n",
    "    df['helpful_perc'] = df['helpful'].apply(calculate_helpful_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "23257\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "df = df_kitchen\n",
    "\n",
    "# parse helpful column into new columns of helpful_votes, total_votes, helpful_perc\n",
    "df['helpful_votes'] = df['helpful'].apply(get_helpful_votes)\n",
    "df['total_votes'] = df['helpful'].apply(get_total_votes)\n",
    "df['helpful_perc'] = df['helpful'].apply(calculate_helpful_perc)\n",
    "\n",
    "\n",
    "# Now create the combinedText with casing and no longer than 512 characters (max of BERT embedding input)\n",
    "df['combinedText'] = df['summary'] + \". \" + df['reviewText']\n",
    "\n",
    "#drop rows where not enough votes exist to discern helpfullness\n",
    "kitchen_df = df.drop(df[df.total_votes < 5].index)\n",
    "balanced_kitchen_df = balance_df(kitchen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(balanced_kitchen_df, test_size = 0.2)\n",
    "df_train, df_val = train_test_split(df_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:109: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1ced4d63494385bf98d3b3271823dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa75c0e5638c444f9b2c2242fac40e99"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9313fdf266e347758e9983f477e8cbd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data to InputExample format, ensure we don't have too many examples to encounter memory issues. Only look at the titles!\n",
    "train_examples = convert_text_to_examples2(df_train['summary'], df_train['helpful_perc'], max_examples=100, texts=None)\n",
    "val_examples = convert_text_to_examples2(df_val['summary'], df_val['helpful_perc'], max_examples=20, texts=None)\n",
    "test_examples = convert_text_to_examples2(df_test['summary'], df_test['helpful_perc'], max_examples=20, texts=None)\n",
    "\n",
    "#make sure the test Y is the same format\n",
    "test_actual = np.array(df_test['helpful_perc'][:20])\n",
    "test_actual = test_actual.reshape(-1,1)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQ_LENGTH)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels \n",
    ") = convert_examples_to_features(tokenizer, val_examples, max_seq_length=MAX_SEQ_LENGTH)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_18 (BertLayer)       (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 128)          98432       bert_layer_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 1)            129         dense_37[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 110,203,451\n",
      "Trainable params: 21,952,769\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-257-7e6177fe21f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#we can now finally build a model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Instantiate variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0minitialize_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m model_BCE_summary.fit(\n",
      "\u001b[1;32m<ipython-input-213-3e68b8e77935>\u001b[0m in \u001b[0;36minitialize_vars\u001b[1;34m(sess)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minitialize_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1346\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1350\u001b[0m                                       target_list, run_metadata)\n",
      "\u001b[1;32mC:\\Users\\Tucker\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1388\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1390\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#let's retry with some regularization\n",
    "model_MSE_summary = build_model(MAX_SEQ_LENGTH, 'mean_squared_error', reg=True)\n",
    "\n",
    "#we can now finally build a model\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model_MSE_summary.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=1,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's retry with some regularization\n",
    "model_BCE_summary = build_model_bert(MAX_SEQ_LENGTH, 'binary_crossentropy', reg=True)\n",
    "\n",
    "#we can now finally build a model\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model_BCE_summary.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=1,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "W266_Final_AmazonReviews.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
